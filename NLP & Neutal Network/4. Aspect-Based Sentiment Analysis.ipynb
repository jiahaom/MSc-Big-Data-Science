{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hExKCzh6doIW"
   },
   "source": [
    "# 4: Aspect-Based Sentiment Analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HixoFOoCIJ7V"
   },
   "source": [
    "We will demonstrate how to deal with the aspect-based sentiment analysis (ABSA), and provide a review text dataset with aspect.\n",
    "Given a review and an aspect, we need to classify the sentiment conveyed towards that aspect on a  three-point scale:   POSITIVE, NEUTRAL, and NEGATIVE.\n",
    "This is a multi-class classification task, and it needs to analyze the text and its aspect. \n",
    "\n",
    "Same as before, we are going to use Keras Sequential API in this session. The Sequential API allows us to make models layer-by-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3493,
     "status": "ok",
     "timestamp": 1647000919548,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "m8fpBfhBpupy"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import LSTM, RNN, Dropout, Input, LeakyReLU, Bidirectional,Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqvPQvgvPv1W"
   },
   "source": [
    "### Downloading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EundMtGPpCdf"
   },
   "source": [
    "Unlike the IMDB dataset that is included and preprocessed by the Keras, the dataset we will be using is the aspect-term sentiment analysis (ATSA) dataset, which consists of 5297 labeled reviews. These are split into 4,297 reviews for training and 500 reviews for testing and validation, respectively. \n",
    "\n",
    "For ATSA, the annotators extract aspect terms in the sentences and label the sentiment polarities with respect to the  aspect  terms.   The  sentences  that  consist  of only one aspect term or multiple aspects with the same  sentiment  polarities  are  deleted.  ATSA also provides the start and end positions in a sentence for each aspect term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1647000920237,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "P27sPFg7f0CJ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def downloadfile(url):\n",
    "    rq = requests.get(url)\n",
    "    open(url.split('/')[-1], 'wb').write(rq.content)\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/train.xml')\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/val.xml')\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/test.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 558,
     "status": "ok",
     "timestamp": 1647000920789,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "X3Voc6S_gT2X",
    "outputId": "6477eee4-4c44-4f34-9467-e465352ba341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 11186\n",
      "Test entries: 1336\n"
     ]
    }
   ],
   "source": [
    "# The code is modified from https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data_process/utils.py\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "def parse_sentence_term(path, lowercase=False):\n",
    "    tree = parse(path)\n",
    "    sentences = tree.getroot()\n",
    "    data = []\n",
    "    split_char = '__split__'\n",
    "    for sentence in sentences:\n",
    "        text = sentence.find('text')\n",
    "        if text is None:\n",
    "            continue\n",
    "        text = text.text\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        aspectTerms = sentence.find('aspectTerms')\n",
    "        if aspectTerms is None:\n",
    "            continue\n",
    "        for aspectTerm in aspectTerms:\n",
    "            term = aspectTerm.get('term')\n",
    "            if lowercase:\n",
    "                term = term.lower()\n",
    "            polarity = aspectTerm.get('polarity')\n",
    "            start = aspectTerm.get('from')\n",
    "            end = aspectTerm.get('to')\n",
    "            piece = [text , term,  polarity , start , end]\n",
    "            data.append(piece)\n",
    "    return data\n",
    "train = parse_sentence_term(\"train.xml\",True)\n",
    "val = parse_sentence_term(\"val.xml\",True)\n",
    "test = parse_sentence_term(\"test.xml\",True)\n",
    "\n",
    "print(\"Training entries: {}\".format(len(train)))\n",
    "print(\"Test entries: {}\".format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U4iCV9-rmay"
   },
   "source": [
    "We now can start playing around with the data, letâ€™s first see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1647000920789,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "h-gjWRAuqg5s",
    "outputId": "e376d985-277e-4586-f552-169d2514e279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE \t ASPECT \t LABEL \t ASPECT-START-INDEX \t ASPECT-END-INDEX\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
      "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
      "['when tables opened up, the manager sat another party before us.', 'manager', 'negative', '27', '34']\n"
     ]
    }
   ],
   "source": [
    "print(\"SENTENCE \\t ASPECT \\t LABEL \\t ASPECT-START-INDEX \\t ASPECT-END-INDEX\")\n",
    "print(train[0])\n",
    "print(train[1])\n",
    "print(train[2])\n",
    "print(train[3])\n",
    "print(train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTRZrpcyr-4x"
   },
   "source": [
    "We could use this dataset to try an \"unknown aspect\" task, if we assume that the ASPECT, LABEL and START/END-INDEX fields are what the model must predict. But here we will attempt a simpler \"known aspect\" task: we will assume that we know ASPECT and START/END-INDEX and the model must just predict the LABEL for a given combination of aspect and sentence.\n",
    "\n",
    "First, build a vocabulary based on the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1647000920789,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "79Ev72Kgq4XL",
    "outputId": "83d55af7-a81b-4944-950d-f50ecf31ea4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7894\n",
      "7898\n"
     ]
    }
   ],
   "source": [
    "voc = []\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "for example in train:\n",
    "    text_tokens = text_to_word_sequence(example[0],\n",
    "                                        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                        lower=True, \n",
    "                                        split=' ')\n",
    "    \n",
    "    aspect_tokens = text_to_word_sequence(example[1],\n",
    "                                         filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                         lower=True, \n",
    "                                         split=' ')\n",
    "    voc.extend(aspect_tokens)\n",
    "    voc.extend(text_tokens)\n",
    "\n",
    "voc = set(voc)\n",
    "print(len(voc))\n",
    "\n",
    "word_index = dict()\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  \n",
    "word_index[\"<EOS>\"] = 3\n",
    "for w in voc:\n",
    "    word_index[w] = len(word_index)\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvuu4KhStqei"
   },
   "source": [
    "According to the word_index and the tokenizer function (text_to_word_sequence), we can convert the review text and aspect words to word tokens and integers separately:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1647000920789,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "gMCH1OoDrSNR",
    "outputId": "5b62745b-a086-469a-f744-e9de045ca126",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_review[0]:\n",
      "['the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it']\n",
      "x_train_aspect[0]:\n",
      "['decor']\n",
      "x_train_review_int[0]:\n",
      "[5178, 6580, 6417, 96, 4190, 6614, 5245, 6264, 6448, 369, 7132, 1098, 875, 1329, 2131, 4903, 1713]\n",
      "x_train_aspect_int[0]:\n",
      "[6580]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert2tokens(dataset, embedding, sent = True):\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "    \n",
    "    for i in dataset:\n",
    "        review = text_to_word_sequence(i[0])\n",
    "        aspect = text_to_word_sequence(i[1])\n",
    "        a.append(review)\n",
    "        b.append(aspect)\n",
    "    \n",
    "        temp1 = []\n",
    "        for j in review:\n",
    "            if j in embedding:\n",
    "                temp1.append(embedding[j])\n",
    "            else:\n",
    "                temp1.append(embedding['<UNK>'])\n",
    "        c.append(temp1)\n",
    "\n",
    "        temp2 =[]\n",
    "        for k in aspect:\n",
    "            if k in embedding:\n",
    "                temp2.append(embedding[k])\n",
    "            else:\n",
    "                temp2.append(embedding['<UNK>'])\n",
    "        d.append(temp2)\n",
    "    if sent == False:\n",
    "        return (c, d)\n",
    "    else:\n",
    "        return (a, b, c, d)\n",
    "\n",
    "x_train_review, x_train_aspect, x_train_review_int, x_train_aspect_int = convert2tokens(train, word_index)\n",
    "x_dev_review, x_dev_aspect, x_dev_review_int, x_dev_aspect_int = convert2tokens(val, word_index)\n",
    "x_test_review, x_test_aspect, x_test_review_int, x_test_aspect_int = convert2tokens(test, word_index)\n",
    "\n",
    "assert len(x_train_aspect) == len(train)\n",
    "assert len(x_train_aspect) == len(x_train_aspect_int)\n",
    "assert len(x_test_aspect) == len(test)\n",
    "assert len(x_test_aspect) == len(x_test_aspect_int)\n",
    "\n",
    "print(\"x_train_review[0]:\")\n",
    "print(x_train_review[0])\n",
    "print(\"x_train_aspect[0]:\")\n",
    "print(x_train_aspect[0])\n",
    "print(\"x_train_review_int[0]:\")\n",
    "print(x_train_review_int[0])\n",
    "print(\"x_train_aspect_int[0]:\")\n",
    "print(x_train_aspect_int[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IreFXgruZot"
   },
   "source": [
    "We use 3 to represent \"positive\", 2 for \"neutral\", and 1 for \"negative\". Then we can convert the lables to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1647000920790,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "abIb7Fe5u3GQ",
    "outputId": "1af1fbb9-6016-4ba8-cdf3-a3a4e3e11750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "[1 0 0]\n",
      "[1 0 0]\n",
      "[0 1 0]\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "def label2int(dataset):\n",
    "    y = []\n",
    "    for example in dataset:\n",
    "        if example[2].lower() == \"negative\":\n",
    "            y.append([0,0,1])\n",
    "        elif example[2].lower() == \"neutral\":\n",
    "            y.append([0,1,0])\n",
    "        else:\n",
    "            y.append([1,0,0])\n",
    "    return y\n",
    "  \n",
    "y_train = label2int(train)\n",
    "y_dev = label2int(val)\n",
    "y_test = label2int(test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train[1])\n",
    "print(y_train[2])\n",
    "print(y_train[3])\n",
    "print(y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnnSuspvC5b"
   },
   "source": [
    "Now we have almost done the data preprocessing. Unlike the previous lab, there are two x (review and aspect) to input the model in here. The easiest way is to combine the review and aspect into one sentence and then input it into the model. Thus we can use the previous model directly.\n",
    "\n",
    "(This means our model is similar to a simplified version of the Vo & Zhang model from the lectures: we have an input sequence containing an aspect embedding paired with the sentence word embeddings (but not separating into left & right sentence context as Vo & Zhang do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1647000921129,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "JjOcmoW01ahB",
    "outputId": "c7094315-2a68-4d61-94db-effa4edd423c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decor']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_aspect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1647000921130,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "Cpmy5TuN1ahB"
   },
   "outputs": [],
   "source": [
    "def review_pad_glove(dataset):\n",
    "    temp = keras.preprocessing.sequence.pad_sequences(dataset,\n",
    "                                                      value=0,\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return temp\n",
    "    \n",
    "\n",
    "\n",
    "def aspect_pad_glove(dataset):\n",
    "    temp = keras.preprocessing.sequence.pad_sequences(dataset,\n",
    "                                                      value=0,\n",
    "                                                      padding='post',\n",
    "                                                      maxlen=16)\n",
    "    return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1647000921532,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "Gk23se111ahB",
    "outputId": "a7659491-053f-4df7-f82d-7218765da8a5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before paded:\n",
      "['decor', '<START>', 'the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it']\n",
      "[6580, 1, 5178, 6580, 6417, 96, 4190, 6614, 5245, 6264, 6448, 369, 7132, 1098, 875, 1329, 2131, 4903, 1713]\n",
      "After paded:\n",
      "[6580    1 5178 6580 6417   96 4190 6614 5245 6264 6448  369 7132 1098\n",
      "  875 1329 2131 4903 1713    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 128\n",
    "\n",
    "def combine_rev_asp (aspect, review):\n",
    "    temp1 = []\n",
    "    temp2 = []\n",
    "    for i in range(len(aspect)):\n",
    "        temp1.append(aspect[i] + ['<START>'] + review[i])\n",
    "\n",
    "    for i in temp1:\n",
    "        temp = []\n",
    "        for j in i:\n",
    "            if j in word_index:\n",
    "                temp.append(word_index[j])\n",
    "            else:\n",
    "                temp.append(word_index['<UNK>'])\n",
    "        temp2.append(temp)\n",
    "        \n",
    "    temp3 =review_pad_glove(temp2)\n",
    "    temp3 = np.array(temp3)\n",
    "    return (temp1, temp2, temp3)\n",
    "        \n",
    "    \n",
    "    \n",
    "x_train, x_train_int, x_train_pad = combine_rev_asp(x_train_aspect, x_train_review)\n",
    "x_dev, x_dev_int, x_dev_pad = combine_rev_asp(x_dev_aspect, x_dev_review)\n",
    "x_test, x_test_int,x_test_pad = combine_rev_asp(x_test_aspect, x_test_review)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Before paded:\")\n",
    "print(x_train[0])\n",
    "print(x_train_int[0])\n",
    "print(\"After paded:\")\n",
    "print(x_train_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7OwOQw4h8RX"
   },
   "source": [
    "# Model 1: Previous models without pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CuAHDGQ3Mmp"
   },
   "source": [
    "## Model 1-1: Neural bag of words without pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1647000921532,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "Yi04MLIvJOGZ"
   },
   "outputs": [],
   "source": [
    "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
    "    def call(self, x, mask=None):\n",
    "        if mask != None:\n",
    "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "        else:\n",
    "            return super().call(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1647000921533,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "JIRRFgNN1ahC"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23532,
     "status": "ok",
     "timestamp": 1647000945059,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "WW6BXvcc1ahC",
    "outputId": "008d717c-eef8-4904-8510-534899b73a3d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 128, 100)          789800    \n",
      "                                                                 \n",
      " global_average_pooling1d_ma  (None, 100)              0         \n",
      " sked (GlobalAveragePooling1                                     \n",
      " DMasked)                                                        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                1616      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 791,467\n",
      "Trainable params: 791,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 3s 36ms/step - loss: 0.6604 - accuracy: 0.4438 - val_loss: 0.6306 - val_accuracy: 0.4535\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 0.6223 - accuracy: 0.4507 - val_loss: 0.6190 - val_accuracy: 0.4535\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6189 - accuracy: 0.4507 - val_loss: 0.6174 - val_accuracy: 0.4535\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.6173 - accuracy: 0.4507 - val_loss: 0.6161 - val_accuracy: 0.4535\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6155 - accuracy: 0.4507 - val_loss: 0.6145 - val_accuracy: 0.4535\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6133 - accuracy: 0.4507 - val_loss: 0.6122 - val_accuracy: 0.4535\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.6104 - accuracy: 0.4507 - val_loss: 0.6091 - val_accuracy: 0.4535\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.6063 - accuracy: 0.4507 - val_loss: 0.6051 - val_accuracy: 0.4535\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.6010 - accuracy: 0.4519 - val_loss: 0.6000 - val_accuracy: 0.4535\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5944 - accuracy: 0.4559 - val_loss: 0.5937 - val_accuracy: 0.4595\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.5865 - accuracy: 0.4738 - val_loss: 0.5871 - val_accuracy: 0.4707\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5781 - accuracy: 0.4991 - val_loss: 0.5807 - val_accuracy: 0.4820\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 0.5698 - accuracy: 0.5190 - val_loss: 0.5748 - val_accuracy: 0.5150\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5616 - accuracy: 0.5232 - val_loss: 0.5690 - val_accuracy: 0.5278\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.5538 - accuracy: 0.5384 - val_loss: 0.5643 - val_accuracy: 0.5270\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.5462 - accuracy: 0.5441 - val_loss: 0.5592 - val_accuracy: 0.5225\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.5391 - accuracy: 0.5570 - val_loss: 0.5549 - val_accuracy: 0.5368\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.5324 - accuracy: 0.5677 - val_loss: 0.5512 - val_accuracy: 0.5405\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.5261 - accuracy: 0.5784 - val_loss: 0.5475 - val_accuracy: 0.5398\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 0.5205 - accuracy: 0.5863 - val_loss: 0.5483 - val_accuracy: 0.5240\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.5153 - accuracy: 0.5862 - val_loss: 0.5430 - val_accuracy: 0.5443\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 0.5097 - accuracy: 0.5973 - val_loss: 0.5396 - val_accuracy: 0.5526\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 1s 29ms/step - loss: 0.5050 - accuracy: 0.6085 - val_loss: 0.5382 - val_accuracy: 0.5518\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 0.5003 - accuracy: 0.6122 - val_loss: 0.5373 - val_accuracy: 0.5450\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.4969 - accuracy: 0.6100 - val_loss: 0.5360 - val_accuracy: 0.5511\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 1s 25ms/step - loss: 0.4922 - accuracy: 0.6209 - val_loss: 0.5342 - val_accuracy: 0.5563\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.4877 - accuracy: 0.6277 - val_loss: 0.5327 - val_accuracy: 0.5683\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.4845 - accuracy: 0.6296 - val_loss: 0.5317 - val_accuracy: 0.5706\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.4808 - accuracy: 0.6336 - val_loss: 0.5311 - val_accuracy: 0.5631\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.4778 - accuracy: 0.6389 - val_loss: 0.5306 - val_accuracy: 0.5646\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5257 - accuracy: 0.5943\n",
      "\n",
      "\n",
      "test_loss: 0.5256696939468384 test_accuracy: 0.5943113565444946\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Embedding(input_dim = VOCAB_SIZE, \n",
    "                      output_dim = 100,\n",
    "                      name='embedding', \n",
    "                      input_length = MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model_1.add(GlobalAveragePooling1DMasked())\n",
    "model_1.add(Dense(16))\n",
    "model_1.add(Dense(3, activation = 'softmax'))\n",
    "model_1.summary()\n",
    "\n",
    "model_1.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history_1 = model_1.fit(x_train_pad,\n",
    "                        y_train,\n",
    "                        epochs=30,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(x_dev_pad, y_dev),\n",
    "                        verbose=1)\n",
    "\n",
    "results_1 = model_1.evaluate(x_test_pad, y_test)\n",
    "print('\\n')\n",
    "print('test_loss:', results_1[0], 'test_accuracy:', results_1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iafDTygK28fv"
   },
   "source": [
    "##  Model 1-2: CNN or LSTM without pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2TnuiKb2-vE"
   },
   "source": [
    "Please try one more model (CNN or LSTM) without pre-trained word embeddings in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84902,
     "status": "ok",
     "timestamp": 1647001029950,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "oXjbq6WcJosQ",
    "outputId": "f8c29e39-e41c-4223-8e18-76e6a78d94b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 128, 100)          789800    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 870,503\n",
      "Trainable params: 870,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 4s 94ms/step - loss: 0.6350 - accuracy: 0.4376 - val_loss: 0.6194 - val_accuracy: 0.4535\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6205 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6194 - val_accuracy: 0.4535\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6205 - accuracy: 0.4507 - val_loss: 0.6194 - val_accuracy: 0.4535\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 1s 65ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 1s 65ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6197 - val_accuracy: 0.4535\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 1s 65ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6197 - val_accuracy: 0.4535\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 1s 67ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 1s 67ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6194 - val_accuracy: 0.4535\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 1s 65ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 1s 65ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6194 - val_accuracy: 0.4535\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6193 - val_accuracy: 0.4535\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 1s 66ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6192 - val_accuracy: 0.4535\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.6193 - accuracy: 0.4543\n",
      "\n",
      "\n",
      "test_loss: 0.6193007230758667 test_accuracy: 0.454341322183609\n"
     ]
    }
   ],
   "source": [
    "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
    "\n",
    "model_1_2 = Sequential()\n",
    "model_1_2.add(Embedding(input_dim = VOCAB_SIZE, \n",
    "                        output_dim = 100,\n",
    "                        name='embedding', \n",
    "                        input_length = MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model_1_2.add(LSTM(100, return_sequences = False))\n",
    "model_1_2.add(Dense(3, activation = 'softmax'))\n",
    "model_1_2.summary()\n",
    "\n",
    "model_1_2.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history_2 = model_1_2.fit(x_train_pad,\n",
    "                          y_train,\n",
    "                          epochs=30,\n",
    "                          batch_size=512,\n",
    "                          validation_data=(x_dev_pad, y_dev),\n",
    "                          verbose=1)\n",
    "\n",
    "results_1_2 = model_1_2.evaluate(x_test_pad, y_test)\n",
    "print('\\n')\n",
    "print('test_loss:', results_1_2[0], 'test_accuracy:', results_1_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--020hfG6rN2"
   },
   "source": [
    "# Model 2: Using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1647001029951,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "gembn7VM3ex8"
   },
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
    "    vocabLen = len(wordToIndex) + 1  \n",
    "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
    "   \n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
    "    for word, index in wordToIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGlove[word] \n",
    "\n",
    "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable, name='GloVe_Embeddings')\n",
    "    return embeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1026118,
     "status": "ok",
     "timestamp": 1647002056508,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "8OC1wuctdFvA",
    "outputId": "3d1a182d-5939-4e65-f180-9dc4fc2d869d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-11 12:17:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2022-03-11 12:17:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-03-11 12:17:10--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: â€˜glove.6B.zip.1â€™\n",
      "\n",
      "glove.6B.zip.1      100%[===================>] 822.24M  4.93MB/s    in 2m 41s  \n",
      "\n",
      "2022-03-11 12:19:52 (5.10 MB/s) - â€˜glove.6B.zip.1â€™ saved [862182613/862182613]\n",
      "\n",
      "Archive:  /content/glove.6B.zip\n",
      "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
      "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
      "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
      "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip '/content/glove.6B.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGxciLK4-xOr"
   },
   "source": [
    "We freeze the weights. To create the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29679,
     "status": "ok",
     "timestamp": 1647002086175,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "PZCPUM0W_Drc",
    "outputId": "41905b5f-c098-4f93-e8a6-723e0e135163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Embedding:  300\n"
     ]
    }
   ],
   "source": [
    "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.50d.txt')\n",
    "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.100d.txt')\n",
    "wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.300d.txt')\n",
    "\n",
    "# import os\n",
    "# data = os.path.expanduser(\"/Users/jiahao/Downloads/Neural Networks and NLP/Week 3/content/glove.6B.300d.txt\")\n",
    "\n",
    "# wordToIndex,indexToWord,wordToGlove=readGloveFile(data)\n",
    "\n",
    "vocabLen = len(wordToIndex) + 1 \n",
    "\n",
    "EMBED_SIZE = next(iter(wordToGlove.values())).shape[0]\n",
    "print('Size of Embedding: ',EMBED_SIZE)\n",
    "\n",
    "embeddingLayer=createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RyeDPimMW7c"
   },
   "source": [
    "### Convert the data to GLOVE word index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_X6nC57NGXv"
   },
   "source": [
    "The index in our vocabulary is different from that in GLOVE. For example, the word \"you\" corresponds to 394475 in GLOVE, while it corresponds to another index in our vocabulary. Thus we can not directly use the index data in the last section. We convert them from text tokens to GLOVE word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1647002086176,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "ODUoIts1NM6b",
    "outputId": "2b9d175a-7875-40a7-faae-858a7e1ee61a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394475\n",
      "you\n",
      "3989\n"
     ]
    }
   ],
   "source": [
    "print(wordToIndex[\"you\"])\n",
    "print(indexToWord[394475])\n",
    "print(word_index[\"you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1647002087179,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "-n_GKK811ahH",
    "outputId": "ffe6b151-ad67-48d5-ebdd-64ec4f79716b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_review_glove[0]:\n",
      "[357266, 118926, 192973, 264550, 338995, 62065, 51582, 87775, 357354, 151204, 54718, 53201, 292136, 231458, 373317, 151349, 193716]\n",
      "x_train_aspect_glove[0]:\n",
      "[118926]\n"
     ]
    }
   ],
   "source": [
    "# GLOVE doesn't have token for unknown words, so assign 0\n",
    "wordToIndex['<UNK>'] = 0\n",
    "\n",
    "x_train_review_glove, x_train_aspect_glove = convert2tokens(train, wordToIndex ,sent = False)\n",
    "x_dev_review_glove, x_dev_aspect_glove = convert2tokens(val, wordToIndex ,sent = False)\n",
    "x_test_review_glove, x_test_aspect_glove = convert2tokens(test, wordToIndex ,sent = False)\n",
    "\n",
    "assert len(x_train_review_glove) == len(train)\n",
    "assert len(x_train_aspect_glove) == len(x_train_aspect_int)\n",
    "assert len(x_test_review_glove) == len(test)\n",
    "assert len(x_test_aspect_glove) == len(x_test_aspect_int)\n",
    "print(\"x_train_review_glove[0]:\")\n",
    "print(x_train_review_glove[0])\n",
    "print(\"x_train_aspect_glove[0]:\")\n",
    "print(x_train_aspect_glove[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z--QOlb1vXtN"
   },
   "source": [
    "As before, we concatenate the tweets and topics for fitting in the previous model. Let us do it again for GLOVE version variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1647002087179,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "uNGRNcOD1ahI",
    "outputId": "46445da7-fccb-4be0-dd54-7e9465d31605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before paded:\n",
      "[118926, 1, 357266, 118926, 192973, 264550, 338995, 62065, 51582, 87775, 357354, 151204, 54718, 53201, 292136, 231458, 373317, 151349, 193716]\n",
      "After paded:\n",
      "[118926      1 357266 118926 192973 264550 338995  62065  51582  87775\n",
      " 357354 151204  54718  53201 292136 231458 373317 151349 193716      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def combine_x (aspect, review):\n",
    "    temp1 = []\n",
    "    for i in range(len(aspect)):\n",
    "        temp1.append(aspect[i] + [1] + review[i])\n",
    "    temp2 = review_pad_glove(temp1)\n",
    "    \n",
    "    # use np.array function to wrap the ouput of pad_sequences function\n",
    "    temp2 = np.array(temp2)\n",
    "    return (temp1, temp2)\n",
    "\n",
    "\n",
    "x_train_glove, x_train_pad_glove = combine_x(x_train_aspect_glove,\n",
    "                                             x_train_review_glove)\n",
    "\n",
    "x_dev_glove, x_dev_pad_glove = combine_x(x_dev_aspect_glove,\n",
    "                                         x_dev_review_glove)\n",
    "\n",
    "x_test_glove, x_test_pad_glove = combine_x(x_test_aspect_glove,\n",
    "                                           x_test_review_glove)\n",
    "\n",
    "\n",
    "# Don't forget the to , such as: x_train_pad = np.array(x_train_pad)\n",
    "# Only pad the *_int varibles\n",
    "\n",
    "print(\"Before paded:\")\n",
    "print(x_train_glove[0])\n",
    "print(\"After paded:\")\n",
    "print(x_train_pad_glove[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdZ4nl08vp9A"
   },
   "source": [
    "## Model 2-1: Neural bag of words using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37113,
     "status": "ok",
     "timestamp": 1647002124285,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "5rO25nIG1ahJ",
    "outputId": "9a174695-dc2a-427c-89d1-9fe2d23381aa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " GloVe_Embeddings (Embedding  (None, None, 300)        120000300 \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling1d_ma  (None, 300)              0         \n",
      " sked_1 (GlobalAveragePoolin                                     \n",
      " g1DMasked)                                                      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                4816      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,005,167\n",
      "Trainable params: 4,867\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "22/22 [==============================] - 1s 15ms/step - loss: 0.6709 - accuracy: 0.4330 - val_loss: 0.6508 - val_accuracy: 0.4557\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.6387 - accuracy: 0.4519 - val_loss: 0.6250 - val_accuracy: 0.4535\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6201 - accuracy: 0.4507 - val_loss: 0.6136 - val_accuracy: 0.4535\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6126 - accuracy: 0.4507 - val_loss: 0.6090 - val_accuracy: 0.4535\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6087 - accuracy: 0.4508 - val_loss: 0.6052 - val_accuracy: 0.4535\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6050 - accuracy: 0.4512 - val_loss: 0.6014 - val_accuracy: 0.4527\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6016 - accuracy: 0.4533 - val_loss: 0.5980 - val_accuracy: 0.4572\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5984 - accuracy: 0.4559 - val_loss: 0.5951 - val_accuracy: 0.4670\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5954 - accuracy: 0.4648 - val_loss: 0.5921 - val_accuracy: 0.4730\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5927 - accuracy: 0.4716 - val_loss: 0.5898 - val_accuracy: 0.4805\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5900 - accuracy: 0.4849 - val_loss: 0.5876 - val_accuracy: 0.4767\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5878 - accuracy: 0.4849 - val_loss: 0.5851 - val_accuracy: 0.4962\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5853 - accuracy: 0.4911 - val_loss: 0.5829 - val_accuracy: 0.4985\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5832 - accuracy: 0.4978 - val_loss: 0.5811 - val_accuracy: 0.4992\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5813 - accuracy: 0.4988 - val_loss: 0.5790 - val_accuracy: 0.5098\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5795 - accuracy: 0.5054 - val_loss: 0.5775 - val_accuracy: 0.5150\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5776 - accuracy: 0.5044 - val_loss: 0.5759 - val_accuracy: 0.5180\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5760 - accuracy: 0.5070 - val_loss: 0.5743 - val_accuracy: 0.5173\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5744 - accuracy: 0.5111 - val_loss: 0.5730 - val_accuracy: 0.5210\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5729 - accuracy: 0.5145 - val_loss: 0.5718 - val_accuracy: 0.5203\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5718 - accuracy: 0.5132 - val_loss: 0.5706 - val_accuracy: 0.5203\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5703 - accuracy: 0.5170 - val_loss: 0.5692 - val_accuracy: 0.5240\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5690 - accuracy: 0.5188 - val_loss: 0.5686 - val_accuracy: 0.5233\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5678 - accuracy: 0.5165 - val_loss: 0.5667 - val_accuracy: 0.5255\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5664 - accuracy: 0.5195 - val_loss: 0.5661 - val_accuracy: 0.5210\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5654 - accuracy: 0.5203 - val_loss: 0.5648 - val_accuracy: 0.5285\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5643 - accuracy: 0.5226 - val_loss: 0.5641 - val_accuracy: 0.5233\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5633 - accuracy: 0.5227 - val_loss: 0.5632 - val_accuracy: 0.5308\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5623 - accuracy: 0.5244 - val_loss: 0.5631 - val_accuracy: 0.5218\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5618 - accuracy: 0.5242 - val_loss: 0.5616 - val_accuracy: 0.5270\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5606 - accuracy: 0.5270 - val_loss: 0.5612 - val_accuracy: 0.5248\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5598 - accuracy: 0.5257 - val_loss: 0.5603 - val_accuracy: 0.5263\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5590 - accuracy: 0.5290 - val_loss: 0.5597 - val_accuracy: 0.5338\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5582 - accuracy: 0.5281 - val_loss: 0.5589 - val_accuracy: 0.5345\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5578 - accuracy: 0.5310 - val_loss: 0.5590 - val_accuracy: 0.5308\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5569 - accuracy: 0.5322 - val_loss: 0.5578 - val_accuracy: 0.5300\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5564 - accuracy: 0.5298 - val_loss: 0.5575 - val_accuracy: 0.5338\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5557 - accuracy: 0.5334 - val_loss: 0.5567 - val_accuracy: 0.5368\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5550 - accuracy: 0.5364 - val_loss: 0.5568 - val_accuracy: 0.5375\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5545 - accuracy: 0.5363 - val_loss: 0.5557 - val_accuracy: 0.5390\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5540 - accuracy: 0.5351 - val_loss: 0.5562 - val_accuracy: 0.5383\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5536 - accuracy: 0.5371 - val_loss: 0.5556 - val_accuracy: 0.5383\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5530 - accuracy: 0.5355 - val_loss: 0.5550 - val_accuracy: 0.5428\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5528 - accuracy: 0.5382 - val_loss: 0.5545 - val_accuracy: 0.5375\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5521 - accuracy: 0.5374 - val_loss: 0.5542 - val_accuracy: 0.5405\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5518 - accuracy: 0.5385 - val_loss: 0.5545 - val_accuracy: 0.5435\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5513 - accuracy: 0.5396 - val_loss: 0.5537 - val_accuracy: 0.5420\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5513 - accuracy: 0.5409 - val_loss: 0.5532 - val_accuracy: 0.5435\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5508 - accuracy: 0.5386 - val_loss: 0.5528 - val_accuracy: 0.5443\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5503 - accuracy: 0.5413 - val_loss: 0.5536 - val_accuracy: 0.5383\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5499 - accuracy: 0.5403 - val_loss: 0.5526 - val_accuracy: 0.5413\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5496 - accuracy: 0.5416 - val_loss: 0.5528 - val_accuracy: 0.5465\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5493 - accuracy: 0.5424 - val_loss: 0.5521 - val_accuracy: 0.5450\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5490 - accuracy: 0.5426 - val_loss: 0.5520 - val_accuracy: 0.5450\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5488 - accuracy: 0.5407 - val_loss: 0.5523 - val_accuracy: 0.5458\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5484 - accuracy: 0.5458 - val_loss: 0.5519 - val_accuracy: 0.5428\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5485 - accuracy: 0.5426 - val_loss: 0.5513 - val_accuracy: 0.5473\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5480 - accuracy: 0.5437 - val_loss: 0.5515 - val_accuracy: 0.5473\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5477 - accuracy: 0.5448 - val_loss: 0.5512 - val_accuracy: 0.5443\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5474 - accuracy: 0.5456 - val_loss: 0.5509 - val_accuracy: 0.5473\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5472 - accuracy: 0.5460 - val_loss: 0.5508 - val_accuracy: 0.5465\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5469 - accuracy: 0.5463 - val_loss: 0.5511 - val_accuracy: 0.5480\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5467 - accuracy: 0.5472 - val_loss: 0.5509 - val_accuracy: 0.5488\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5464 - accuracy: 0.5487 - val_loss: 0.5508 - val_accuracy: 0.5518\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5463 - accuracy: 0.5464 - val_loss: 0.5508 - val_accuracy: 0.5450\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5460 - accuracy: 0.5466 - val_loss: 0.5513 - val_accuracy: 0.5480\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5458 - accuracy: 0.5481 - val_loss: 0.5506 - val_accuracy: 0.5458\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5456 - accuracy: 0.5479 - val_loss: 0.5505 - val_accuracy: 0.5541\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5454 - accuracy: 0.5470 - val_loss: 0.5500 - val_accuracy: 0.5488\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5454 - accuracy: 0.5472 - val_loss: 0.5501 - val_accuracy: 0.5503\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5452 - accuracy: 0.5493 - val_loss: 0.5506 - val_accuracy: 0.5503\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5451 - accuracy: 0.5500 - val_loss: 0.5503 - val_accuracy: 0.5503\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5447 - accuracy: 0.5476 - val_loss: 0.5498 - val_accuracy: 0.5495\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5449 - accuracy: 0.5494 - val_loss: 0.5498 - val_accuracy: 0.5511\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5445 - accuracy: 0.5476 - val_loss: 0.5500 - val_accuracy: 0.5556\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5443 - accuracy: 0.5516 - val_loss: 0.5500 - val_accuracy: 0.5511\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5442 - accuracy: 0.5500 - val_loss: 0.5497 - val_accuracy: 0.5526\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5440 - accuracy: 0.5502 - val_loss: 0.5497 - val_accuracy: 0.5526\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5440 - accuracy: 0.5508 - val_loss: 0.5497 - val_accuracy: 0.5533\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.5438 - accuracy: 0.5521 - val_loss: 0.5499 - val_accuracy: 0.5541\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5436 - accuracy: 0.5508 - val_loss: 0.5504 - val_accuracy: 0.5541\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5434 - accuracy: 0.5502 - val_loss: 0.5495 - val_accuracy: 0.5518\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5434 - accuracy: 0.5527 - val_loss: 0.5497 - val_accuracy: 0.5548\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5434 - accuracy: 0.5487 - val_loss: 0.5503 - val_accuracy: 0.5556\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5432 - accuracy: 0.5516 - val_loss: 0.5495 - val_accuracy: 0.5548\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5431 - accuracy: 0.5535 - val_loss: 0.5506 - val_accuracy: 0.5518\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5430 - accuracy: 0.5525 - val_loss: 0.5494 - val_accuracy: 0.5526\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5429 - accuracy: 0.5530 - val_loss: 0.5492 - val_accuracy: 0.5548\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5427 - accuracy: 0.5527 - val_loss: 0.5493 - val_accuracy: 0.5556\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5427 - accuracy: 0.5541 - val_loss: 0.5495 - val_accuracy: 0.5548\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5425 - accuracy: 0.5506 - val_loss: 0.5498 - val_accuracy: 0.5616\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5424 - accuracy: 0.5520 - val_loss: 0.5494 - val_accuracy: 0.5541\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5423 - accuracy: 0.5534 - val_loss: 0.5495 - val_accuracy: 0.5593\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5422 - accuracy: 0.5514 - val_loss: 0.5496 - val_accuracy: 0.5548\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5420 - accuracy: 0.5518 - val_loss: 0.5494 - val_accuracy: 0.5571\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5423 - accuracy: 0.5548 - val_loss: 0.5501 - val_accuracy: 0.5608\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5419 - accuracy: 0.5537 - val_loss: 0.5498 - val_accuracy: 0.5533\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5418 - accuracy: 0.5521 - val_loss: 0.5500 - val_accuracy: 0.5556\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5420 - accuracy: 0.5529 - val_loss: 0.5494 - val_accuracy: 0.5586\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5416 - accuracy: 0.5531 - val_loss: 0.5494 - val_accuracy: 0.5571\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5416 - accuracy: 0.5527 - val_loss: 0.5496 - val_accuracy: 0.5533\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5415 - accuracy: 0.5522 - val_loss: 0.5493 - val_accuracy: 0.5586\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5413 - accuracy: 0.5534 - val_loss: 0.5499 - val_accuracy: 0.5593\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5413 - accuracy: 0.5547 - val_loss: 0.5495 - val_accuracy: 0.5548\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5412 - accuracy: 0.5536 - val_loss: 0.5496 - val_accuracy: 0.5578\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5414 - accuracy: 0.5546 - val_loss: 0.5495 - val_accuracy: 0.5533\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5416 - accuracy: 0.5530 - val_loss: 0.5492 - val_accuracy: 0.5511\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5412 - accuracy: 0.5530 - val_loss: 0.5502 - val_accuracy: 0.5571\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5412 - accuracy: 0.5575 - val_loss: 0.5494 - val_accuracy: 0.5548\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5408 - accuracy: 0.5542 - val_loss: 0.5492 - val_accuracy: 0.5556\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5408 - accuracy: 0.5545 - val_loss: 0.5494 - val_accuracy: 0.5631\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5408 - accuracy: 0.5556 - val_loss: 0.5494 - val_accuracy: 0.5533\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5410 - accuracy: 0.5522 - val_loss: 0.5496 - val_accuracy: 0.5623\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5405 - accuracy: 0.5538 - val_loss: 0.5498 - val_accuracy: 0.5571\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5406 - accuracy: 0.5552 - val_loss: 0.5496 - val_accuracy: 0.5586\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5404 - accuracy: 0.5561 - val_loss: 0.5499 - val_accuracy: 0.5601\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5404 - accuracy: 0.5561 - val_loss: 0.5497 - val_accuracy: 0.5593\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5403 - accuracy: 0.5571 - val_loss: 0.5500 - val_accuracy: 0.5631\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5406 - accuracy: 0.5536 - val_loss: 0.5507 - val_accuracy: 0.5593\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5406 - accuracy: 0.5564 - val_loss: 0.5496 - val_accuracy: 0.5616\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5401 - accuracy: 0.5576 - val_loss: 0.5500 - val_accuracy: 0.5563\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5400 - accuracy: 0.5561 - val_loss: 0.5499 - val_accuracy: 0.5623\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5402 - accuracy: 0.5579 - val_loss: 0.5501 - val_accuracy: 0.5586\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5402 - accuracy: 0.5571 - val_loss: 0.5501 - val_accuracy: 0.5563\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5399 - accuracy: 0.5585 - val_loss: 0.5494 - val_accuracy: 0.5601\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5401 - accuracy: 0.5574 - val_loss: 0.5500 - val_accuracy: 0.5631\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5398 - accuracy: 0.5568 - val_loss: 0.5501 - val_accuracy: 0.5586\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5398 - accuracy: 0.5584 - val_loss: 0.5495 - val_accuracy: 0.5578\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5398 - accuracy: 0.5586 - val_loss: 0.5504 - val_accuracy: 0.5563\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5402 - accuracy: 0.5560 - val_loss: 0.5501 - val_accuracy: 0.5601\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5396 - accuracy: 0.5562 - val_loss: 0.5500 - val_accuracy: 0.5631\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5398 - accuracy: 0.5544 - val_loss: 0.5498 - val_accuracy: 0.5653\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5397 - accuracy: 0.5587 - val_loss: 0.5498 - val_accuracy: 0.5623\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5397 - accuracy: 0.5562 - val_loss: 0.5495 - val_accuracy: 0.5616\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5395 - accuracy: 0.5555 - val_loss: 0.5496 - val_accuracy: 0.5586\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5393 - accuracy: 0.5586 - val_loss: 0.5501 - val_accuracy: 0.5608\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5395 - accuracy: 0.5567 - val_loss: 0.5496 - val_accuracy: 0.5593\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5392 - accuracy: 0.5561 - val_loss: 0.5501 - val_accuracy: 0.5646\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5394 - accuracy: 0.5554 - val_loss: 0.5498 - val_accuracy: 0.5631\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5393 - accuracy: 0.5582 - val_loss: 0.5503 - val_accuracy: 0.5586\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5392 - accuracy: 0.5585 - val_loss: 0.5500 - val_accuracy: 0.5616\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5392 - accuracy: 0.5564 - val_loss: 0.5499 - val_accuracy: 0.5571\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5390 - accuracy: 0.5574 - val_loss: 0.5507 - val_accuracy: 0.5638\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5392 - accuracy: 0.5584 - val_loss: 0.5505 - val_accuracy: 0.5586\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5396 - accuracy: 0.5576 - val_loss: 0.5517 - val_accuracy: 0.5541\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5396 - accuracy: 0.5569 - val_loss: 0.5508 - val_accuracy: 0.5616\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5392 - accuracy: 0.5567 - val_loss: 0.5498 - val_accuracy: 0.5616\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5390 - accuracy: 0.5550 - val_loss: 0.5499 - val_accuracy: 0.5646\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5395 - accuracy: 0.5593 - val_loss: 0.5516 - val_accuracy: 0.5541\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5389 - accuracy: 0.5596 - val_loss: 0.5498 - val_accuracy: 0.5601\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5389 - accuracy: 0.5583 - val_loss: 0.5502 - val_accuracy: 0.5608\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5388 - accuracy: 0.5581 - val_loss: 0.5504 - val_accuracy: 0.5601\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5389 - accuracy: 0.5595 - val_loss: 0.5504 - val_accuracy: 0.5593\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5389 - accuracy: 0.5572 - val_loss: 0.5498 - val_accuracy: 0.5623\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5388 - accuracy: 0.5600 - val_loss: 0.5505 - val_accuracy: 0.5571\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5388 - accuracy: 0.5581 - val_loss: 0.5503 - val_accuracy: 0.5631\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5388 - accuracy: 0.5565 - val_loss: 0.5506 - val_accuracy: 0.5578\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5388 - accuracy: 0.5569 - val_loss: 0.5500 - val_accuracy: 0.5593\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5386 - accuracy: 0.5598 - val_loss: 0.5499 - val_accuracy: 0.5616\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5386 - accuracy: 0.5561 - val_loss: 0.5503 - val_accuracy: 0.5623\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5386 - accuracy: 0.5581 - val_loss: 0.5506 - val_accuracy: 0.5601\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5387 - accuracy: 0.5568 - val_loss: 0.5501 - val_accuracy: 0.5578\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5386 - accuracy: 0.5587 - val_loss: 0.5499 - val_accuracy: 0.5631\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5386 - accuracy: 0.5578 - val_loss: 0.5498 - val_accuracy: 0.5631\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5384 - accuracy: 0.5586 - val_loss: 0.5505 - val_accuracy: 0.5601\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5383 - accuracy: 0.5593 - val_loss: 0.5499 - val_accuracy: 0.5653\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5383 - accuracy: 0.5586 - val_loss: 0.5501 - val_accuracy: 0.5593\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5382 - accuracy: 0.5594 - val_loss: 0.5507 - val_accuracy: 0.5608\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5383 - accuracy: 0.5572 - val_loss: 0.5502 - val_accuracy: 0.5691\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5383 - accuracy: 0.5584 - val_loss: 0.5511 - val_accuracy: 0.5578\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5385 - accuracy: 0.5587 - val_loss: 0.5512 - val_accuracy: 0.5541\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5383 - accuracy: 0.5564 - val_loss: 0.5501 - val_accuracy: 0.5616\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5384 - accuracy: 0.5601 - val_loss: 0.5502 - val_accuracy: 0.5601\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5382 - accuracy: 0.5556 - val_loss: 0.5501 - val_accuracy: 0.5608\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5381 - accuracy: 0.5587 - val_loss: 0.5504 - val_accuracy: 0.5593\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5382 - accuracy: 0.5589 - val_loss: 0.5505 - val_accuracy: 0.5631\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5380 - accuracy: 0.5580 - val_loss: 0.5507 - val_accuracy: 0.5661\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5383 - accuracy: 0.5589 - val_loss: 0.5502 - val_accuracy: 0.5706\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5382 - accuracy: 0.5586 - val_loss: 0.5502 - val_accuracy: 0.5608\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5380 - accuracy: 0.5603 - val_loss: 0.5506 - val_accuracy: 0.5623\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5381 - accuracy: 0.5581 - val_loss: 0.5501 - val_accuracy: 0.5623\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5382 - accuracy: 0.5586 - val_loss: 0.5508 - val_accuracy: 0.5653\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5379 - accuracy: 0.5574 - val_loss: 0.5502 - val_accuracy: 0.5616\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5380 - accuracy: 0.5586 - val_loss: 0.5516 - val_accuracy: 0.5556\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5383 - accuracy: 0.5571 - val_loss: 0.5521 - val_accuracy: 0.5465\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5385 - accuracy: 0.5569 - val_loss: 0.5509 - val_accuracy: 0.5578\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5380 - accuracy: 0.5603 - val_loss: 0.5502 - val_accuracy: 0.5601\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5380 - accuracy: 0.5575 - val_loss: 0.5506 - val_accuracy: 0.5623\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5378 - accuracy: 0.5561 - val_loss: 0.5503 - val_accuracy: 0.5623\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5377 - accuracy: 0.5582 - val_loss: 0.5507 - val_accuracy: 0.5683\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5379 - accuracy: 0.5573 - val_loss: 0.5503 - val_accuracy: 0.5691\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5378 - accuracy: 0.5590 - val_loss: 0.5501 - val_accuracy: 0.5631\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5377 - accuracy: 0.5603 - val_loss: 0.5517 - val_accuracy: 0.5586\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5377 - accuracy: 0.5584 - val_loss: 0.5505 - val_accuracy: 0.5616\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5377 - accuracy: 0.5580 - val_loss: 0.5504 - val_accuracy: 0.5638\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5376 - accuracy: 0.5586 - val_loss: 0.5511 - val_accuracy: 0.5631\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5377 - accuracy: 0.5567 - val_loss: 0.5503 - val_accuracy: 0.5616\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5375 - accuracy: 0.5596 - val_loss: 0.5508 - val_accuracy: 0.5593\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5376 - accuracy: 0.5580 - val_loss: 0.5508 - val_accuracy: 0.5608\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5376 - accuracy: 0.5581 - val_loss: 0.5505 - val_accuracy: 0.5638\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.5372 - accuracy: 0.5704\n",
      "\n",
      "\n",
      "test_loss: 0.5372355580329895 test_accuracy: 0.5703592896461487\n"
     ]
    }
   ],
   "source": [
    "model_2_1 = Sequential()\n",
    "\n",
    "# an embedding layer. \n",
    "model_2_1.add(embeddingLayer)\n",
    "model_2_1.add(GlobalAveragePooling1DMasked())\n",
    "model_2_1.add(Dense(16))\n",
    "model_2_1.add(Dense(3, activation= \"softmax\"))\n",
    "model_2_1.summary()\n",
    "\n",
    "model_2_1.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history_2_1 = model_2_1.fit(x_train_pad_glove,\n",
    "                       y_train,\n",
    "                       epochs=200,\n",
    "                       batch_size=512,\n",
    "                       validation_data=(x_dev_pad_glove, y_dev),\n",
    "                       verbose=1)\n",
    "\n",
    "results_2_1 = model_2_1.evaluate(x_test_pad_glove, y_test)\n",
    "print('\\n')\n",
    "print('test_loss:', results_2_1[0], 'test_accuracy:', results_2_1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aTWUWQS2VFd"
   },
   "source": [
    "The accuracy is around 56%. In this version, the \"glorot_uniform\" initialization method does not improve model performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ1KWFKvcagS"
   },
   "source": [
    "##  Model 2-2: CNN or LSTM with pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYubGfP-2VEL"
   },
   "source": [
    "Please try one more model (CNN or LSTM) with pre-trained word embeddings in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56233,
     "status": "ok",
     "timestamp": 1647002180507,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "KeLTrJ-3zKBj",
    "outputId": "2174f631-1459-48dc-894b-3f5fa9e6be82",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " GloVe_Embeddings (Embedding  (None, None, 300)        120000300 \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               160400    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,161,003\n",
      "Trainable params: 160,703\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 4s 107ms/step - loss: 0.6496 - accuracy: 0.4477 - val_loss: 0.6208 - val_accuracy: 0.4535\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6203 - accuracy: 0.4507 - val_loss: 0.6191 - val_accuracy: 0.4535\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6202 - accuracy: 0.4507 - val_loss: 0.6191 - val_accuracy: 0.4535\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6198 - accuracy: 0.4508 - val_loss: 0.6183 - val_accuracy: 0.4535\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.6168 - accuracy: 0.4526 - val_loss: 0.6069 - val_accuracy: 0.4827\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 2s 83ms/step - loss: 0.6093 - accuracy: 0.4689 - val_loss: 0.6162 - val_accuracy: 0.4842\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.6133 - accuracy: 0.4958 - val_loss: 0.6158 - val_accuracy: 0.4692\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6063 - accuracy: 0.4879 - val_loss: 0.5772 - val_accuracy: 0.5435\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.5765 - accuracy: 0.5459 - val_loss: 0.5742 - val_accuracy: 0.5480\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.5758 - accuracy: 0.5463 - val_loss: 0.5765 - val_accuracy: 0.5450\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.5766 - accuracy: 0.5458 - val_loss: 0.5759 - val_accuracy: 0.5450\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.5760 - accuracy: 0.5456 - val_loss: 0.5763 - val_accuracy: 0.5450\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.5762 - accuracy: 0.5457 - val_loss: 0.5764 - val_accuracy: 0.5450\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 2s 82ms/step - loss: 0.5762 - accuracy: 0.5458 - val_loss: 0.5757 - val_accuracy: 0.5450\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.5764 - accuracy: 0.5454 - val_loss: 0.5762 - val_accuracy: 0.5450\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.5763 - accuracy: 0.5454 - val_loss: 0.5759 - val_accuracy: 0.5450\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.5763 - accuracy: 0.5456 - val_loss: 0.5762 - val_accuracy: 0.5450\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.5946 - accuracy: 0.5088 - val_loss: 0.6166 - val_accuracy: 0.4610\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6195 - accuracy: 0.4538 - val_loss: 0.6190 - val_accuracy: 0.4542\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6194 - accuracy: 0.4527 - val_loss: 0.6189 - val_accuracy: 0.4542\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6193 - accuracy: 0.4526 - val_loss: 0.6189 - val_accuracy: 0.4542\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 2s 81ms/step - loss: 0.6192 - accuracy: 0.4527 - val_loss: 0.6189 - val_accuracy: 0.4542\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6194 - accuracy: 0.4527 - val_loss: 0.6191 - val_accuracy: 0.4542\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6193 - accuracy: 0.4526 - val_loss: 0.6189 - val_accuracy: 0.4542\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6193 - accuracy: 0.4527 - val_loss: 0.6190 - val_accuracy: 0.4542\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6193 - accuracy: 0.4527 - val_loss: 0.6189 - val_accuracy: 0.4542\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6193 - accuracy: 0.4527 - val_loss: 0.6189 - val_accuracy: 0.4542\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.6193 - accuracy: 0.4527 - val_loss: 0.6189 - val_accuracy: 0.4542\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 2s 78ms/step - loss: 0.6193 - accuracy: 0.4527 - val_loss: 0.6189 - val_accuracy: 0.4542\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 0.6193 - accuracy: 0.4527 - val_loss: 0.6190 - val_accuracy: 0.4542\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.6186 - accuracy: 0.4558\n",
      "\n",
      "\n",
      "test_loss: 0.6186115741729736 test_accuracy: 0.45583832263946533\n"
     ]
    }
   ],
   "source": [
    "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
    "\n",
    "model2_2 = Sequential()\n",
    "model2_2.add(embeddingLayer)\n",
    "model2_2.add(LSTM(100, return_sequences = False))\n",
    "\n",
    "model2_2.add(Dense(3, activation = 'softmax'))\n",
    "model2_2.summary()\n",
    "\n",
    "model2_2.compile(optimizer='adam',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history_2_2 = model2_2.fit(x_train_pad_glove,\n",
    "                           y_train,\n",
    "                           epochs=30,\n",
    "                           batch_size=512,\n",
    "                           validation_data=(x_dev_pad_glove, y_dev),\n",
    "                           verbose=1)\n",
    "\n",
    "results_2_2 = model2_2.evaluate(x_test_pad_glove, y_test)\n",
    "print('\\n')\n",
    "print('test_loss:', results_2_2[0], 'test_accuracy:', results_2_2[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-bZ5SCHiIMl"
   },
   "source": [
    "#  Model 3: Model with multiple-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G85QM3lSV7qp"
   },
   "source": [
    "In models 1 and 2, we combine the reviews and aspects to input into the models. In model 3, we separately input these two data into the model and use different layers to analyze them. \n",
    "\n",
    "(This will give us a model similar to a simplified version of the Xue & Li model from the lectures - we have a separate paths through the network for the aspect embedding and the sentence, being combined - but we don't have to use gating like Xue & Li)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1647002180508,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "ztiFcOWuA0xH"
   },
   "outputs": [],
   "source": [
    "# First of all, pad the review and aspect separately\n",
    "x_train_review_pad_glove = review_pad_glove(x_train_review_glove)\n",
    "x_dev_review_pad_glove = review_pad_glove(x_dev_review_glove)\n",
    "x_test_review_pad_glove = review_pad_glove(x_test_review_glove)\n",
    "\n",
    "\n",
    "x_train_aspect_pad_glove = aspect_pad_glove(x_train_aspect_glove)\n",
    "x_dev_aspect_pad_glove = aspect_pad_glove(x_dev_aspect_glove)\n",
    "x_test_aspect_pad_glove = aspect_pad_glove(x_test_aspect_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExgX8bxpVgps"
   },
   "source": [
    "## Model 3-1 Neural bag of words model with multiple-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-TN6yup01mC"
   },
   "source": [
    "Model 3-1 needs us to modify the model 2-1 to be compatible with multiple-input.\n",
    "We could find some tutorial examples from (https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/).\n",
    "\n",
    "Please print the model summary and visualize it using vis_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1647002180508,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "D0YjUDlE1ahK",
    "outputId": "0c6e1825-c6f0-4d07-8608-0505aa51ffb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_maske  (None, 300)         0           ['GloVe_Embeddings[2][0]']       \n",
      " d_2 (GlobalAveragePooling1DMas                                                                   \n",
      " ked)                                                                                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_maske  (None, 300)         0           ['GloVe_Embeddings[3][0]']       \n",
      " d_3 (GlobalAveragePooling1DMas                                                                   \n",
      " ked)                                                                                             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           4816        ['global_average_pooling1d_masked\n",
      "                                                                 _2[0][0]']                       \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 16)           4816        ['global_average_pooling1d_masked\n",
      "                                                                 _3[0][0]']                       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32)           0           ['dense_6[0][0]',                \n",
      "                                                                  'dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 3)            99          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 120,010,031\n",
      "Trainable params: 9,731\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(16, ))\n",
    "input2 = Input(shape=(128,))\n",
    "embed1 = embeddingLayer(input1)\n",
    "embed2 = embeddingLayer(input2)\n",
    "avgpool_1 = GlobalAveragePooling1DMasked()(embed1)\n",
    "avgpool_2 = GlobalAveragePooling1DMasked()(embed2)\n",
    "\n",
    "dense1 = Dense(16)(avgpool_1)\n",
    "dense2 = Dense(16)(avgpool_2)\n",
    "concatenate = keras.layers.Concatenate()([dense1, dense2])\n",
    "output = Dense(3, activation = 'softmax')(concatenate)\n",
    "\n",
    "model3_1 = keras.models.Model(inputs=[input1, input2], outputs = output)\n",
    "model3_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1647002180997,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "UlJbDRHeAMIh",
    "outputId": "7b5e7a8e-7382-4211-cdc8-d67d3d7df969"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"392pt\" viewBox=\"0.00 0.00 1017.50 470.00\" width=\"848pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 1013.5,-466 1013.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139822843158480 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139822843158480</title>\n",
       "<polygon fill=\"none\" points=\"175,-415.5 175,-461.5 491,-461.5 491,-415.5 175,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215\" y=\"-446.3\">input_1</text>\n",
       "<polyline fill=\"none\" points=\"175,-438.5 255,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"215\" y=\"-423.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"255,-415.5 255,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"284\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"255,-438.5 313,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"284\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"313,-415.5 313,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"357.5\" y=\"-434.8\">[(None, 16)]</text>\n",
       "<polyline fill=\"none\" points=\"402,-415.5 402,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"446.5\" y=\"-434.8\">[(None, 16)]</text>\n",
       "</g>\n",
       "<!-- 139822879539472 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139822879539472</title>\n",
       "<polygon fill=\"none\" points=\"384,-332.5 384,-378.5 622,-378.5 622,-332.5 384,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"451\" y=\"-363.3\">GloVe_Embeddings</text>\n",
       "<polyline fill=\"none\" points=\"384,-355.5 518,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"451\" y=\"-340.3\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"518,-332.5 518,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"518,-355.5 576,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"576,-332.5 576,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"587.5\" y=\"-351.8\">?</text>\n",
       "<polyline fill=\"none\" points=\"599,-332.5 599,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"610.5\" y=\"-351.8\">?</text>\n",
       "</g>\n",
       "<!-- 139822843158480&#45;&gt;139822879539472 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139822843158480-&gt;139822879539472</title>\n",
       "<path d=\"M380.3545,-415.3799C400.9,-405.3488 425.1229,-393.5224 446.5915,-383.0406\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"448.1544,-386.1725 455.6049,-378.6399 445.0832,-379.8822 448.1544,-386.1725\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822843158416 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139822843158416</title>\n",
       "<polygon fill=\"none\" points=\"509,-415.5 509,-461.5 839,-461.5 839,-415.5 509,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"549\" y=\"-446.3\">input_2</text>\n",
       "<polyline fill=\"none\" points=\"509,-438.5 589,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"549\" y=\"-423.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"589,-415.5 589,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"589,-438.5 647,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"618\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"647,-415.5 647,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"695\" y=\"-434.8\">[(None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"743,-415.5 743,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"791\" y=\"-434.8\">[(None, 128)]</text>\n",
       "</g>\n",
       "<!-- 139822843158416&#45;&gt;139822879539472 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139822843158416-&gt;139822879539472</title>\n",
       "<path d=\"M626.367,-415.3799C605.7006,-405.3488 581.3352,-393.5224 559.7403,-383.0406\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"561.1985,-379.8579 550.6739,-378.6399 558.1418,-386.1553 561.1985,-379.8579\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822865836816 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139822865836816</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 492,-295.5 492,-249.5 0,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118.5\" y=\"-280.3\">global_average_pooling1d_masked_2</text>\n",
       "<polyline fill=\"none\" points=\"0,-272.5 237,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118.5\" y=\"-257.3\">GlobalAveragePooling1DMasked</text>\n",
       "<polyline fill=\"none\" points=\"237,-249.5 237,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"237,-272.5 295,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"295,-249.5 295,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"350\" y=\"-268.8\">(None, 16, 300)</text>\n",
       "<polyline fill=\"none\" points=\"405,-249.5 405,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"448.5\" y=\"-268.8\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 139822879539472&#45;&gt;139822865836816 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139822879539472-&gt;139822865836816</title>\n",
       "<path d=\"M431.7525,-332.4901C399.3035,-322.0105 360.732,-309.5535 327.1684,-298.7139\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"327.9778,-295.2974 317.3861,-295.5547 325.8265,-301.9586 327.9778,-295.2974\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139825724248592 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139825724248592</title>\n",
       "<polygon fill=\"none\" points=\"510.5,-249.5 510.5,-295.5 1009.5,-295.5 1009.5,-249.5 510.5,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"629\" y=\"-280.3\">global_average_pooling1d_masked_3</text>\n",
       "<polyline fill=\"none\" points=\"510.5,-272.5 747.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"629\" y=\"-257.3\">GlobalAveragePooling1DMasked</text>\n",
       "<polyline fill=\"none\" points=\"747.5,-249.5 747.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"776.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"747.5,-272.5 805.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"776.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"805.5,-249.5 805.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"864\" y=\"-268.8\">(None, 128, 300)</text>\n",
       "<polyline fill=\"none\" points=\"922.5,-249.5 922.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"966\" y=\"-268.8\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 139822879539472&#45;&gt;139825724248592 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139822879539472-&gt;139825724248592</title>\n",
       "<path d=\"M574.2475,-332.4901C606.6965,-322.0105 645.268,-309.5535 678.8316,-298.7139\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"680.1735,-301.9586 688.6139,-295.5547 678.0222,-295.2974 680.1735,-301.9586\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822849580176 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139822849580176</title>\n",
       "<polygon fill=\"none\" points=\"204.5,-166.5 204.5,-212.5 493.5,-212.5 493.5,-166.5 204.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-197.3\">dense_6</text>\n",
       "<polyline fill=\"none\" points=\"204.5,-189.5 268.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236.5\" y=\"-174.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"268.5,-166.5 268.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"268.5,-189.5 326.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"326.5,-166.5 326.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370\" y=\"-185.8\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"413.5,-166.5 413.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"453.5\" y=\"-185.8\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 139822865836816&#45;&gt;139822849580176 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139822865836816-&gt;139822849580176</title>\n",
       "<path d=\"M274.6912,-249.3799C286.3126,-240.0151 299.8758,-229.0855 312.2102,-219.1462\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"314.515,-221.7839 320.1054,-212.784 310.1227,-216.3333 314.515,-221.7839\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822851735184 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139822851735184</title>\n",
       "<polygon fill=\"none\" points=\"563.5,-166.5 563.5,-212.5 852.5,-212.5 852.5,-166.5 563.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595.5\" y=\"-197.3\">dense_7</text>\n",
       "<polyline fill=\"none\" points=\"563.5,-189.5 627.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595.5\" y=\"-174.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"627.5,-166.5 627.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"656.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"627.5,-189.5 685.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"656.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"685.5,-166.5 685.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"729\" y=\"-185.8\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"772.5,-166.5 772.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"812.5\" y=\"-185.8\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 139825724248592&#45;&gt;139822851735184 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>139825724248592-&gt;139822851735184</title>\n",
       "<path d=\"M745.5151,-249.3799C740.095,-240.7286 733.8383,-230.7419 728.0069,-221.4341\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"730.8628,-219.4 722.5876,-212.784 724.9308,-223.1165 730.8628,-219.4\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822841763280 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>139822841763280</title>\n",
       "<polygon fill=\"none\" points=\"310.5,-83.5 310.5,-129.5 693.5,-129.5 693.5,-83.5 310.5,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-114.3\">concatenate</text>\n",
       "<polyline fill=\"none\" points=\"310.5,-106.5 396.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-91.3\">Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"396.5,-83.5 396.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"425.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"396.5,-106.5 454.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"425.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"454.5,-83.5 454.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"534\" y=\"-102.8\">[(None, 16), (None, 16)]</text>\n",
       "<polyline fill=\"none\" points=\"613.5,-83.5 613.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"653.5\" y=\"-102.8\">(None, 32)</text>\n",
       "</g>\n",
       "<!-- 139822849580176&#45;&gt;139822841763280 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>139822849580176-&gt;139822841763280</title>\n",
       "<path d=\"M391.619,-166.3799C409.8624,-156.4832 431.3273,-144.8388 450.4547,-134.4625\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"452.2235,-137.4849 459.3444,-129.6399 448.8856,-131.3319 452.2235,-137.4849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822851735184&#45;&gt;139822841763280 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>139822851735184-&gt;139822841763280</title>\n",
       "<path d=\"M650.8911,-166.4901C625.4395,-156.2353 595.2888,-144.0872 568.7985,-133.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"569.8035,-130.0455 559.22,-129.5547 567.1874,-136.5383 569.8035,-130.0455\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139825616048016 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>139825616048016</title>\n",
       "<polygon fill=\"none\" points=\"365,-.5 365,-46.5 639,-46.5 639,-.5 365,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397\" y=\"-31.3\">dense_8</text>\n",
       "<polyline fill=\"none\" points=\"365,-23.5 429,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397\" y=\"-8.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"429,-.5 429,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"458\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"429,-23.5 487,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"458\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"487,-.5 487,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"527\" y=\"-19.8\">(None, 32)</text>\n",
       "<polyline fill=\"none\" points=\"567,-.5 567,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"603\" y=\"-19.8\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 139822841763280&#45;&gt;139825616048016 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>139822841763280-&gt;139825616048016</title>\n",
       "<path d=\"M502,-83.3799C502,-75.1745 502,-65.7679 502,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"505.5001,-56.784 502,-46.784 498.5001,-56.784 505.5001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import vis_utils\n",
    "SVG(vis_utils.model_to_dot(model3_1, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVl_cKt903Z9"
   },
   "source": [
    "Train and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39605,
     "status": "ok",
     "timestamp": 1647002220580,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "u1bxFpIJ1ahK",
    "outputId": "49ed22e7-4c2f-462c-90e9-fa8741c7d983",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "22/22 [==============================] - 1s 16ms/step - loss: 0.6591 - accuracy: 0.4461 - val_loss: 0.6275 - val_accuracy: 0.4535\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6158 - accuracy: 0.4507 - val_loss: 0.6045 - val_accuracy: 0.4535\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.6017 - accuracy: 0.4507 - val_loss: 0.5941 - val_accuracy: 0.4542\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5918 - accuracy: 0.4560 - val_loss: 0.5839 - val_accuracy: 0.4775\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5820 - accuracy: 0.4969 - val_loss: 0.5739 - val_accuracy: 0.5233\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5722 - accuracy: 0.5367 - val_loss: 0.5640 - val_accuracy: 0.5495\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5628 - accuracy: 0.5639 - val_loss: 0.5546 - val_accuracy: 0.5691\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5539 - accuracy: 0.5738 - val_loss: 0.5458 - val_accuracy: 0.5803\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5459 - accuracy: 0.5813 - val_loss: 0.5381 - val_accuracy: 0.5871\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5388 - accuracy: 0.5876 - val_loss: 0.5313 - val_accuracy: 0.5946\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5329 - accuracy: 0.5891 - val_loss: 0.5255 - val_accuracy: 0.5961\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.5277 - accuracy: 0.5938 - val_loss: 0.5208 - val_accuracy: 0.6021\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5234 - accuracy: 0.5954 - val_loss: 0.5168 - val_accuracy: 0.6089\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5196 - accuracy: 0.5998 - val_loss: 0.5138 - val_accuracy: 0.6036\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5162 - accuracy: 0.6007 - val_loss: 0.5103 - val_accuracy: 0.6209\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5131 - accuracy: 0.6028 - val_loss: 0.5073 - val_accuracy: 0.6216\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5103 - accuracy: 0.6083 - val_loss: 0.5050 - val_accuracy: 0.6254\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5078 - accuracy: 0.6106 - val_loss: 0.5028 - val_accuracy: 0.6276\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5054 - accuracy: 0.6132 - val_loss: 0.5008 - val_accuracy: 0.6276\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.5032 - accuracy: 0.6133 - val_loss: 0.4987 - val_accuracy: 0.6291\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.5011 - accuracy: 0.6173 - val_loss: 0.4971 - val_accuracy: 0.6314\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4991 - accuracy: 0.6188 - val_loss: 0.4953 - val_accuracy: 0.6306\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4973 - accuracy: 0.6210 - val_loss: 0.4938 - val_accuracy: 0.6321\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4959 - accuracy: 0.6235 - val_loss: 0.4929 - val_accuracy: 0.6306\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4939 - accuracy: 0.6259 - val_loss: 0.4913 - val_accuracy: 0.6329\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4928 - accuracy: 0.6261 - val_loss: 0.4902 - val_accuracy: 0.6299\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4907 - accuracy: 0.6282 - val_loss: 0.4888 - val_accuracy: 0.6396\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4894 - accuracy: 0.6278 - val_loss: 0.4882 - val_accuracy: 0.6374\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4881 - accuracy: 0.6297 - val_loss: 0.4869 - val_accuracy: 0.6411\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4867 - accuracy: 0.6297 - val_loss: 0.4864 - val_accuracy: 0.6419\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4859 - accuracy: 0.6313 - val_loss: 0.4852 - val_accuracy: 0.6441\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4848 - accuracy: 0.6324 - val_loss: 0.4852 - val_accuracy: 0.6396\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4834 - accuracy: 0.6345 - val_loss: 0.4836 - val_accuracy: 0.6434\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4823 - accuracy: 0.6363 - val_loss: 0.4833 - val_accuracy: 0.6449\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4813 - accuracy: 0.6376 - val_loss: 0.4824 - val_accuracy: 0.6486\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4805 - accuracy: 0.6395 - val_loss: 0.4822 - val_accuracy: 0.6449\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4795 - accuracy: 0.6423 - val_loss: 0.4815 - val_accuracy: 0.6449\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4787 - accuracy: 0.6432 - val_loss: 0.4809 - val_accuracy: 0.6494\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4778 - accuracy: 0.6437 - val_loss: 0.4805 - val_accuracy: 0.6502\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4769 - accuracy: 0.6463 - val_loss: 0.4798 - val_accuracy: 0.6479\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4763 - accuracy: 0.6462 - val_loss: 0.4801 - val_accuracy: 0.6434\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4757 - accuracy: 0.6471 - val_loss: 0.4793 - val_accuracy: 0.6486\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4750 - accuracy: 0.6474 - val_loss: 0.4789 - val_accuracy: 0.6494\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4747 - accuracy: 0.6484 - val_loss: 0.4784 - val_accuracy: 0.6486\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4737 - accuracy: 0.6492 - val_loss: 0.4784 - val_accuracy: 0.6509\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4739 - accuracy: 0.6477 - val_loss: 0.4784 - val_accuracy: 0.6449\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4727 - accuracy: 0.6492 - val_loss: 0.4780 - val_accuracy: 0.6449\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4720 - accuracy: 0.6495 - val_loss: 0.4773 - val_accuracy: 0.6532\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4716 - accuracy: 0.6510 - val_loss: 0.4772 - val_accuracy: 0.6486\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4711 - accuracy: 0.6517 - val_loss: 0.4771 - val_accuracy: 0.6494\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4707 - accuracy: 0.6519 - val_loss: 0.4777 - val_accuracy: 0.6464\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4701 - accuracy: 0.6530 - val_loss: 0.4765 - val_accuracy: 0.6547\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4696 - accuracy: 0.6532 - val_loss: 0.4770 - val_accuracy: 0.6494\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4693 - accuracy: 0.6530 - val_loss: 0.4766 - val_accuracy: 0.6524\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4687 - accuracy: 0.6549 - val_loss: 0.4761 - val_accuracy: 0.6539\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4684 - accuracy: 0.6534 - val_loss: 0.4760 - val_accuracy: 0.6532\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4682 - accuracy: 0.6539 - val_loss: 0.4759 - val_accuracy: 0.6577\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4680 - accuracy: 0.6548 - val_loss: 0.4757 - val_accuracy: 0.6509\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4673 - accuracy: 0.6561 - val_loss: 0.4759 - val_accuracy: 0.6479\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4668 - accuracy: 0.6543 - val_loss: 0.4755 - val_accuracy: 0.6509\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4666 - accuracy: 0.6555 - val_loss: 0.4765 - val_accuracy: 0.6532\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4663 - accuracy: 0.6563 - val_loss: 0.4755 - val_accuracy: 0.6554\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4658 - accuracy: 0.6565 - val_loss: 0.4756 - val_accuracy: 0.6532\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4657 - accuracy: 0.6558 - val_loss: 0.4754 - val_accuracy: 0.6547\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4655 - accuracy: 0.6563 - val_loss: 0.4753 - val_accuracy: 0.6554\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4650 - accuracy: 0.6571 - val_loss: 0.4752 - val_accuracy: 0.6494\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4650 - accuracy: 0.6572 - val_loss: 0.4752 - val_accuracy: 0.6517\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4644 - accuracy: 0.6571 - val_loss: 0.4749 - val_accuracy: 0.6562\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4642 - accuracy: 0.6586 - val_loss: 0.4756 - val_accuracy: 0.6456\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4640 - accuracy: 0.6590 - val_loss: 0.4756 - val_accuracy: 0.6471\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4635 - accuracy: 0.6590 - val_loss: 0.4754 - val_accuracy: 0.6486\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4637 - accuracy: 0.6592 - val_loss: 0.4757 - val_accuracy: 0.6441\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4632 - accuracy: 0.6596 - val_loss: 0.4751 - val_accuracy: 0.6532\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4629 - accuracy: 0.6606 - val_loss: 0.4749 - val_accuracy: 0.6554\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4628 - accuracy: 0.6598 - val_loss: 0.4748 - val_accuracy: 0.6569\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4628 - accuracy: 0.6594 - val_loss: 0.4757 - val_accuracy: 0.6419\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4622 - accuracy: 0.6615 - val_loss: 0.4748 - val_accuracy: 0.6592\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4621 - accuracy: 0.6596 - val_loss: 0.4747 - val_accuracy: 0.6614\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4618 - accuracy: 0.6598 - val_loss: 0.4751 - val_accuracy: 0.6554\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4615 - accuracy: 0.6612 - val_loss: 0.4750 - val_accuracy: 0.6532\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4615 - accuracy: 0.6614 - val_loss: 0.4748 - val_accuracy: 0.6592\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4614 - accuracy: 0.6612 - val_loss: 0.4755 - val_accuracy: 0.6449\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4611 - accuracy: 0.6624 - val_loss: 0.4754 - val_accuracy: 0.6434\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4609 - accuracy: 0.6631 - val_loss: 0.4763 - val_accuracy: 0.6471\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4608 - accuracy: 0.6620 - val_loss: 0.4751 - val_accuracy: 0.6554\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4605 - accuracy: 0.6632 - val_loss: 0.4753 - val_accuracy: 0.6494\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4602 - accuracy: 0.6617 - val_loss: 0.4758 - val_accuracy: 0.6434\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4601 - accuracy: 0.6613 - val_loss: 0.4755 - val_accuracy: 0.6456\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4599 - accuracy: 0.6634 - val_loss: 0.4750 - val_accuracy: 0.6517\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4598 - accuracy: 0.6634 - val_loss: 0.4755 - val_accuracy: 0.6486\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4595 - accuracy: 0.6642 - val_loss: 0.4762 - val_accuracy: 0.6434\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4595 - accuracy: 0.6639 - val_loss: 0.4752 - val_accuracy: 0.6517\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4593 - accuracy: 0.6641 - val_loss: 0.4756 - val_accuracy: 0.6486\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4591 - accuracy: 0.6645 - val_loss: 0.4754 - val_accuracy: 0.6494\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4587 - accuracy: 0.6637 - val_loss: 0.4753 - val_accuracy: 0.6479\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4587 - accuracy: 0.6636 - val_loss: 0.4753 - val_accuracy: 0.6524\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4588 - accuracy: 0.6642 - val_loss: 0.4751 - val_accuracy: 0.6502\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4584 - accuracy: 0.6631 - val_loss: 0.4755 - val_accuracy: 0.6502\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4583 - accuracy: 0.6652 - val_loss: 0.4754 - val_accuracy: 0.6502\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4580 - accuracy: 0.6649 - val_loss: 0.4759 - val_accuracy: 0.6449\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4581 - accuracy: 0.6656 - val_loss: 0.4755 - val_accuracy: 0.6479\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4580 - accuracy: 0.6643 - val_loss: 0.4756 - val_accuracy: 0.6502\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4576 - accuracy: 0.6645 - val_loss: 0.4758 - val_accuracy: 0.6456\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4577 - accuracy: 0.6647 - val_loss: 0.4766 - val_accuracy: 0.6449\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4575 - accuracy: 0.6648 - val_loss: 0.4759 - val_accuracy: 0.6494\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4575 - accuracy: 0.6656 - val_loss: 0.4758 - val_accuracy: 0.6479\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4572 - accuracy: 0.6658 - val_loss: 0.4766 - val_accuracy: 0.6456\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4573 - accuracy: 0.6654 - val_loss: 0.4757 - val_accuracy: 0.6486\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4569 - accuracy: 0.6638 - val_loss: 0.4760 - val_accuracy: 0.6479\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4571 - accuracy: 0.6665 - val_loss: 0.4760 - val_accuracy: 0.6502\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4568 - accuracy: 0.6659 - val_loss: 0.4768 - val_accuracy: 0.6456\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4570 - accuracy: 0.6653 - val_loss: 0.4765 - val_accuracy: 0.6464\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4565 - accuracy: 0.6659 - val_loss: 0.4760 - val_accuracy: 0.6509\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4567 - accuracy: 0.6661 - val_loss: 0.4760 - val_accuracy: 0.6464\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4566 - accuracy: 0.6662 - val_loss: 0.4764 - val_accuracy: 0.6479\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.4561 - accuracy: 0.6663 - val_loss: 0.4762 - val_accuracy: 0.6494\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4562 - accuracy: 0.6661 - val_loss: 0.4764 - val_accuracy: 0.6494\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4563 - accuracy: 0.6674 - val_loss: 0.4763 - val_accuracy: 0.6532\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4560 - accuracy: 0.6664 - val_loss: 0.4764 - val_accuracy: 0.6532\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4560 - accuracy: 0.6659 - val_loss: 0.4767 - val_accuracy: 0.6449\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4559 - accuracy: 0.6654 - val_loss: 0.4769 - val_accuracy: 0.6456\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4557 - accuracy: 0.6661 - val_loss: 0.4763 - val_accuracy: 0.6494\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4558 - accuracy: 0.6652 - val_loss: 0.4764 - val_accuracy: 0.6532\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4559 - accuracy: 0.6677 - val_loss: 0.4768 - val_accuracy: 0.6554\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4557 - accuracy: 0.6665 - val_loss: 0.4768 - val_accuracy: 0.6502\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4555 - accuracy: 0.6661 - val_loss: 0.4770 - val_accuracy: 0.6456\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4552 - accuracy: 0.6674 - val_loss: 0.4768 - val_accuracy: 0.6502\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4551 - accuracy: 0.6661 - val_loss: 0.4781 - val_accuracy: 0.6381\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4553 - accuracy: 0.6666 - val_loss: 0.4772 - val_accuracy: 0.6479\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4551 - accuracy: 0.6666 - val_loss: 0.4771 - val_accuracy: 0.6441\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4550 - accuracy: 0.6665 - val_loss: 0.4774 - val_accuracy: 0.6479\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4548 - accuracy: 0.6673 - val_loss: 0.4775 - val_accuracy: 0.6456\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4552 - accuracy: 0.6673 - val_loss: 0.4775 - val_accuracy: 0.6434\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4548 - accuracy: 0.6688 - val_loss: 0.4775 - val_accuracy: 0.6517\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4548 - accuracy: 0.6676 - val_loss: 0.4776 - val_accuracy: 0.6456\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4545 - accuracy: 0.6674 - val_loss: 0.4776 - val_accuracy: 0.6449\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4545 - accuracy: 0.6678 - val_loss: 0.4776 - val_accuracy: 0.6456\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4546 - accuracy: 0.6676 - val_loss: 0.4777 - val_accuracy: 0.6441\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4546 - accuracy: 0.6698 - val_loss: 0.4777 - val_accuracy: 0.6562\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4547 - accuracy: 0.6691 - val_loss: 0.4773 - val_accuracy: 0.6524\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4542 - accuracy: 0.6682 - val_loss: 0.4781 - val_accuracy: 0.6419\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4542 - accuracy: 0.6680 - val_loss: 0.4777 - val_accuracy: 0.6449\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4540 - accuracy: 0.6691 - val_loss: 0.4774 - val_accuracy: 0.6569\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4540 - accuracy: 0.6702 - val_loss: 0.4779 - val_accuracy: 0.6486\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4540 - accuracy: 0.6689 - val_loss: 0.4778 - val_accuracy: 0.6532\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4537 - accuracy: 0.6691 - val_loss: 0.4779 - val_accuracy: 0.6449\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4537 - accuracy: 0.6686 - val_loss: 0.4779 - val_accuracy: 0.6441\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4537 - accuracy: 0.6685 - val_loss: 0.4776 - val_accuracy: 0.6502\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4544 - accuracy: 0.6701 - val_loss: 0.4777 - val_accuracy: 0.6532\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4536 - accuracy: 0.6688 - val_loss: 0.4798 - val_accuracy: 0.6396\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4537 - accuracy: 0.6689 - val_loss: 0.4778 - val_accuracy: 0.6524\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4534 - accuracy: 0.6685 - val_loss: 0.4779 - val_accuracy: 0.6509\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4533 - accuracy: 0.6690 - val_loss: 0.4777 - val_accuracy: 0.6524\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4532 - accuracy: 0.6694 - val_loss: 0.4786 - val_accuracy: 0.6464\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4530 - accuracy: 0.6704 - val_loss: 0.4781 - val_accuracy: 0.6607\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4537 - accuracy: 0.6709 - val_loss: 0.4782 - val_accuracy: 0.6539\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4534 - accuracy: 0.6688 - val_loss: 0.4780 - val_accuracy: 0.6517\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4533 - accuracy: 0.6691 - val_loss: 0.4780 - val_accuracy: 0.6539\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4531 - accuracy: 0.6697 - val_loss: 0.4780 - val_accuracy: 0.6509\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4531 - accuracy: 0.6703 - val_loss: 0.4782 - val_accuracy: 0.6494\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4533 - accuracy: 0.6689 - val_loss: 0.4782 - val_accuracy: 0.6486\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4528 - accuracy: 0.6711 - val_loss: 0.4784 - val_accuracy: 0.6509\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4526 - accuracy: 0.6703 - val_loss: 0.4783 - val_accuracy: 0.6471\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4529 - accuracy: 0.6705 - val_loss: 0.4783 - val_accuracy: 0.6524\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4528 - accuracy: 0.6707 - val_loss: 0.4782 - val_accuracy: 0.6517\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4530 - accuracy: 0.6680 - val_loss: 0.4782 - val_accuracy: 0.6502\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4529 - accuracy: 0.6700 - val_loss: 0.4786 - val_accuracy: 0.6494\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4526 - accuracy: 0.6708 - val_loss: 0.4787 - val_accuracy: 0.6532\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4526 - accuracy: 0.6694 - val_loss: 0.4788 - val_accuracy: 0.6471\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4525 - accuracy: 0.6710 - val_loss: 0.4792 - val_accuracy: 0.6456\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4524 - accuracy: 0.6700 - val_loss: 0.4793 - val_accuracy: 0.6449\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4525 - accuracy: 0.6715 - val_loss: 0.4788 - val_accuracy: 0.6464\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4523 - accuracy: 0.6686 - val_loss: 0.4786 - val_accuracy: 0.6517\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4524 - accuracy: 0.6693 - val_loss: 0.4784 - val_accuracy: 0.6554\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4523 - accuracy: 0.6711 - val_loss: 0.4787 - val_accuracy: 0.6532\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4522 - accuracy: 0.6706 - val_loss: 0.4791 - val_accuracy: 0.6471\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4521 - accuracy: 0.6717 - val_loss: 0.4790 - val_accuracy: 0.6486\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4522 - accuracy: 0.6707 - val_loss: 0.4792 - val_accuracy: 0.6479\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4520 - accuracy: 0.6701 - val_loss: 0.4789 - val_accuracy: 0.6494\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4520 - accuracy: 0.6704 - val_loss: 0.4788 - val_accuracy: 0.6517\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4522 - accuracy: 0.6700 - val_loss: 0.4791 - val_accuracy: 0.6569\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4524 - accuracy: 0.6699 - val_loss: 0.4789 - val_accuracy: 0.6532\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4520 - accuracy: 0.6691 - val_loss: 0.4790 - val_accuracy: 0.6562\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4518 - accuracy: 0.6700 - val_loss: 0.4796 - val_accuracy: 0.6441\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4521 - accuracy: 0.6682 - val_loss: 0.4805 - val_accuracy: 0.6419\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4521 - accuracy: 0.6696 - val_loss: 0.4799 - val_accuracy: 0.6479\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4517 - accuracy: 0.6697 - val_loss: 0.4804 - val_accuracy: 0.6441\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4516 - accuracy: 0.6706 - val_loss: 0.4795 - val_accuracy: 0.6524\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4518 - accuracy: 0.6700 - val_loss: 0.4789 - val_accuracy: 0.6562\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4518 - accuracy: 0.6707 - val_loss: 0.4793 - val_accuracy: 0.6502\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4516 - accuracy: 0.6712 - val_loss: 0.4791 - val_accuracy: 0.6562\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4515 - accuracy: 0.6707 - val_loss: 0.4792 - val_accuracy: 0.6547\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4518 - accuracy: 0.6707 - val_loss: 0.4791 - val_accuracy: 0.6524\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4516 - accuracy: 0.6711 - val_loss: 0.4793 - val_accuracy: 0.6532\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4515 - accuracy: 0.6708 - val_loss: 0.4795 - val_accuracy: 0.6502\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4514 - accuracy: 0.6721 - val_loss: 0.4794 - val_accuracy: 0.6562\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.4514 - accuracy: 0.6699 - val_loss: 0.4797 - val_accuracy: 0.6486\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4512 - accuracy: 0.6718 - val_loss: 0.4795 - val_accuracy: 0.6471\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4511 - accuracy: 0.6728 - val_loss: 0.4794 - val_accuracy: 0.6532\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.4515 - accuracy: 0.6695 - val_loss: 0.4794 - val_accuracy: 0.6547\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.4717 - accuracy: 0.6519\n",
      "\n",
      "\n",
      "test_loss: 0.4717029333114624 test_accuracy: 0.6519461274147034\n"
     ]
    }
   ],
   "source": [
    "model3_1.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "history_3_1 = model3_1.fit([x_train_aspect_pad_glove,\n",
    "                           x_train_review_pad_glove],\n",
    "                           y_train,\n",
    "                           epochs=200,\n",
    "                           batch_size=512,\n",
    "                           validation_data=([x_dev_aspect_pad_glove, x_dev_review_pad_glove], y_dev),\n",
    "                           verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "results_3_1 = model3_1.evaluate([x_test_aspect_pad_glove,\n",
    "                                 x_test_review_pad_glove], y_test)\n",
    "print('\\n')\n",
    "print('test_loss:', results_3_1[0], 'test_accuracy:', results_3_1[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0npTvFuVt5R"
   },
   "source": [
    "## Model 3-2 CNN or LSTM model with multiple-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCp9khOn0D2-"
   },
   "source": [
    "Modify the previous CNN or LSTM model to be compatible with multiple-input, similar to model 3-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400437,
     "status": "ok",
     "timestamp": 1647002620996,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "Js5yZqmYywev",
    "outputId": "b4da1fa7-4134-4d7a-bce3-4d9026437c41",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 100)          160400      ['GloVe_Embeddings[4][0]']       \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 100)          160400      ['GloVe_Embeddings[5][0]']       \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 16)           1616        ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 16)           1616        ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32)           0           ['dense_9[0][0]',                \n",
      "                                                                  'dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 3)            99          ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 120,324,431\n",
      "Trainable params: 324,131\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "22/22 [==============================] - 7s 138ms/step - loss: 0.6353 - accuracy: 0.3812 - val_loss: 0.5831 - val_accuracy: 0.5195\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.5489 - accuracy: 0.5606 - val_loss: 0.5220 - val_accuracy: 0.6014\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.5209 - accuracy: 0.5990 - val_loss: 0.5111 - val_accuracy: 0.6081\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.5143 - accuracy: 0.6040 - val_loss: 0.5013 - val_accuracy: 0.6149\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.5055 - accuracy: 0.6128 - val_loss: 0.4983 - val_accuracy: 0.6186\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.5016 - accuracy: 0.6154 - val_loss: 0.4983 - val_accuracy: 0.6299\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.5003 - accuracy: 0.6193 - val_loss: 0.4979 - val_accuracy: 0.6254\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4981 - accuracy: 0.6216 - val_loss: 0.4976 - val_accuracy: 0.6231\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4953 - accuracy: 0.6253 - val_loss: 0.4944 - val_accuracy: 0.6336\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.4936 - accuracy: 0.6280 - val_loss: 0.4962 - val_accuracy: 0.6284\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4917 - accuracy: 0.6303 - val_loss: 0.4952 - val_accuracy: 0.6321\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4880 - accuracy: 0.6372 - val_loss: 0.4984 - val_accuracy: 0.6314\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4858 - accuracy: 0.6370 - val_loss: 0.4941 - val_accuracy: 0.6344\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4831 - accuracy: 0.6414 - val_loss: 0.4981 - val_accuracy: 0.6276\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4796 - accuracy: 0.6456 - val_loss: 0.5022 - val_accuracy: 0.6344\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4770 - accuracy: 0.6496 - val_loss: 0.4991 - val_accuracy: 0.6404\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4783 - accuracy: 0.6481 - val_loss: 0.4979 - val_accuracy: 0.6284\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4725 - accuracy: 0.6558 - val_loss: 0.5029 - val_accuracy: 0.6351\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4684 - accuracy: 0.6573 - val_loss: 0.5052 - val_accuracy: 0.6344\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4689 - accuracy: 0.6598 - val_loss: 0.5040 - val_accuracy: 0.6329\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4644 - accuracy: 0.6623 - val_loss: 0.5202 - val_accuracy: 0.6209\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4637 - accuracy: 0.6620 - val_loss: 0.5079 - val_accuracy: 0.6411\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4586 - accuracy: 0.6685 - val_loss: 0.5153 - val_accuracy: 0.6269\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4549 - accuracy: 0.6722 - val_loss: 0.5212 - val_accuracy: 0.6179\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 2s 92ms/step - loss: 0.4501 - accuracy: 0.6780 - val_loss: 0.5230 - val_accuracy: 0.6111\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4518 - accuracy: 0.6712 - val_loss: 0.5264 - val_accuracy: 0.6216\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4481 - accuracy: 0.6747 - val_loss: 0.5267 - val_accuracy: 0.6254\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4449 - accuracy: 0.6779 - val_loss: 0.5207 - val_accuracy: 0.6261\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 2s 92ms/step - loss: 0.4401 - accuracy: 0.6819 - val_loss: 0.5364 - val_accuracy: 0.6246\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4361 - accuracy: 0.6816 - val_loss: 0.5520 - val_accuracy: 0.6231\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4331 - accuracy: 0.6854 - val_loss: 0.5517 - val_accuracy: 0.6246\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4337 - accuracy: 0.6835 - val_loss: 0.5448 - val_accuracy: 0.6284\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4291 - accuracy: 0.6855 - val_loss: 0.5598 - val_accuracy: 0.6246\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4277 - accuracy: 0.6866 - val_loss: 0.5484 - val_accuracy: 0.6336\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4305 - accuracy: 0.6874 - val_loss: 0.5512 - val_accuracy: 0.6299\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4246 - accuracy: 0.6920 - val_loss: 0.5761 - val_accuracy: 0.6216\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4235 - accuracy: 0.6908 - val_loss: 0.5723 - val_accuracy: 0.6246\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4232 - accuracy: 0.6871 - val_loss: 0.5762 - val_accuracy: 0.6126\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4200 - accuracy: 0.6904 - val_loss: 0.5784 - val_accuracy: 0.6171\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4213 - accuracy: 0.6927 - val_loss: 0.5731 - val_accuracy: 0.6314\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4192 - accuracy: 0.6915 - val_loss: 0.5799 - val_accuracy: 0.6246\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4191 - accuracy: 0.6910 - val_loss: 0.5734 - val_accuracy: 0.6299\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4167 - accuracy: 0.6927 - val_loss: 0.5841 - val_accuracy: 0.6291\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4144 - accuracy: 0.6967 - val_loss: 0.5925 - val_accuracy: 0.6351\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4100 - accuracy: 0.6993 - val_loss: 0.5911 - val_accuracy: 0.6231\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4178 - accuracy: 0.6917 - val_loss: 0.5499 - val_accuracy: 0.6186\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4202 - accuracy: 0.6910 - val_loss: 0.5841 - val_accuracy: 0.6276\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4140 - accuracy: 0.6951 - val_loss: 0.5743 - val_accuracy: 0.6284\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4111 - accuracy: 0.6969 - val_loss: 0.5906 - val_accuracy: 0.6269\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4084 - accuracy: 0.6993 - val_loss: 0.6101 - val_accuracy: 0.6299\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4065 - accuracy: 0.6960 - val_loss: 0.5995 - val_accuracy: 0.6306\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4049 - accuracy: 0.7009 - val_loss: 0.6031 - val_accuracy: 0.6299\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4035 - accuracy: 0.7036 - val_loss: 0.5994 - val_accuracy: 0.6284\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4062 - accuracy: 0.7003 - val_loss: 0.6211 - val_accuracy: 0.6299\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4069 - accuracy: 0.6967 - val_loss: 0.6233 - val_accuracy: 0.6179\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4045 - accuracy: 0.6995 - val_loss: 0.6215 - val_accuracy: 0.6231\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4071 - accuracy: 0.6974 - val_loss: 0.6182 - val_accuracy: 0.6209\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4063 - accuracy: 0.6985 - val_loss: 0.6393 - val_accuracy: 0.6111\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4085 - accuracy: 0.6990 - val_loss: 0.6144 - val_accuracy: 0.6261\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.4069 - accuracy: 0.6986 - val_loss: 0.6198 - val_accuracy: 0.6254\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4049 - accuracy: 0.7009 - val_loss: 0.6279 - val_accuracy: 0.6201\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4021 - accuracy: 0.6996 - val_loss: 0.6055 - val_accuracy: 0.6209\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4003 - accuracy: 0.7028 - val_loss: 0.6351 - val_accuracy: 0.6239\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3992 - accuracy: 0.7054 - val_loss: 0.6150 - val_accuracy: 0.6261\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3990 - accuracy: 0.7037 - val_loss: 0.6359 - val_accuracy: 0.6254\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3985 - accuracy: 0.7020 - val_loss: 0.6277 - val_accuracy: 0.6291\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3973 - accuracy: 0.7047 - val_loss: 0.6485 - val_accuracy: 0.6201\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4012 - accuracy: 0.6992 - val_loss: 0.6494 - val_accuracy: 0.6194\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4013 - accuracy: 0.7008 - val_loss: 0.6496 - val_accuracy: 0.6111\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.4009 - accuracy: 0.7009 - val_loss: 0.6290 - val_accuracy: 0.6276\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4003 - accuracy: 0.7018 - val_loss: 0.6444 - val_accuracy: 0.6216\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3974 - accuracy: 0.7064 - val_loss: 0.6686 - val_accuracy: 0.6246\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3971 - accuracy: 0.7047 - val_loss: 0.6691 - val_accuracy: 0.6239\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3942 - accuracy: 0.7092 - val_loss: 0.6591 - val_accuracy: 0.6276\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3945 - accuracy: 0.7060 - val_loss: 0.6639 - val_accuracy: 0.6149\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3942 - accuracy: 0.7031 - val_loss: 0.6755 - val_accuracy: 0.6231\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3933 - accuracy: 0.7045 - val_loss: 0.6608 - val_accuracy: 0.6269\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3946 - accuracy: 0.7078 - val_loss: 0.6807 - val_accuracy: 0.6284\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3963 - accuracy: 0.7045 - val_loss: 0.6484 - val_accuracy: 0.6179\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 2s 92ms/step - loss: 0.3945 - accuracy: 0.7071 - val_loss: 0.6962 - val_accuracy: 0.6269\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3920 - accuracy: 0.7082 - val_loss: 0.6891 - val_accuracy: 0.6246\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3922 - accuracy: 0.7060 - val_loss: 0.6846 - val_accuracy: 0.6284\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3925 - accuracy: 0.7064 - val_loss: 0.6787 - val_accuracy: 0.6239\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3907 - accuracy: 0.7064 - val_loss: 0.6880 - val_accuracy: 0.6261\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3919 - accuracy: 0.7080 - val_loss: 0.6784 - val_accuracy: 0.6261\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3970 - accuracy: 0.7060 - val_loss: 0.6807 - val_accuracy: 0.6254\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3959 - accuracy: 0.7069 - val_loss: 0.6668 - val_accuracy: 0.6201\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3939 - accuracy: 0.7077 - val_loss: 0.6899 - val_accuracy: 0.6216\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3910 - accuracy: 0.7069 - val_loss: 0.6911 - val_accuracy: 0.6261\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.3899 - accuracy: 0.7085 - val_loss: 0.7125 - val_accuracy: 0.6246\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3902 - accuracy: 0.7070 - val_loss: 0.7213 - val_accuracy: 0.6231\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3894 - accuracy: 0.7101 - val_loss: 0.7014 - val_accuracy: 0.6276\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3884 - accuracy: 0.7077 - val_loss: 0.7211 - val_accuracy: 0.6284\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.3877 - accuracy: 0.7088 - val_loss: 0.7079 - val_accuracy: 0.6321\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3878 - accuracy: 0.7081 - val_loss: 0.7092 - val_accuracy: 0.6269\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3883 - accuracy: 0.7082 - val_loss: 0.7337 - val_accuracy: 0.6269\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.3870 - accuracy: 0.7080 - val_loss: 0.7328 - val_accuracy: 0.6284\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.3870 - accuracy: 0.7090 - val_loss: 0.7359 - val_accuracy: 0.6201\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3880 - accuracy: 0.7104 - val_loss: 0.7355 - val_accuracy: 0.6284\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3893 - accuracy: 0.7085 - val_loss: 0.7259 - val_accuracy: 0.6209\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3915 - accuracy: 0.7091 - val_loss: 0.7185 - val_accuracy: 0.6299\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4179 - accuracy: 0.6927 - val_loss: 0.6190 - val_accuracy: 0.6239\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4023 - accuracy: 0.7030 - val_loss: 0.6905 - val_accuracy: 0.6291\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 2s 92ms/step - loss: 0.3949 - accuracy: 0.7061 - val_loss: 0.6638 - val_accuracy: 0.6314\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3916 - accuracy: 0.7074 - val_loss: 0.6691 - val_accuracy: 0.6321\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.3889 - accuracy: 0.7074 - val_loss: 0.6952 - val_accuracy: 0.6291\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3885 - accuracy: 0.7089 - val_loss: 0.7032 - val_accuracy: 0.6374\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 2s 92ms/step - loss: 0.3873 - accuracy: 0.7087 - val_loss: 0.6958 - val_accuracy: 0.6269\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3864 - accuracy: 0.7087 - val_loss: 0.7110 - val_accuracy: 0.6314\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3875 - accuracy: 0.7107 - val_loss: 0.7088 - val_accuracy: 0.6186\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3867 - accuracy: 0.7108 - val_loss: 0.7107 - val_accuracy: 0.6186\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3874 - accuracy: 0.7081 - val_loss: 0.7282 - val_accuracy: 0.6284\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3878 - accuracy: 0.7098 - val_loss: 0.6875 - val_accuracy: 0.6246\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3884 - accuracy: 0.7086 - val_loss: 0.7173 - val_accuracy: 0.6306\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3876 - accuracy: 0.7100 - val_loss: 0.7132 - val_accuracy: 0.6351\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.3896 - accuracy: 0.7072 - val_loss: 0.7165 - val_accuracy: 0.6261\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.3931 - accuracy: 0.7100 - val_loss: 0.7237 - val_accuracy: 0.6179\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3891 - accuracy: 0.7101 - val_loss: 0.7387 - val_accuracy: 0.6179\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3890 - accuracy: 0.7080 - val_loss: 0.7090 - val_accuracy: 0.6224\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.3882 - accuracy: 0.7092 - val_loss: 0.7034 - val_accuracy: 0.6216\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3867 - accuracy: 0.7102 - val_loss: 0.7290 - val_accuracy: 0.6351\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3867 - accuracy: 0.7090 - val_loss: 0.7215 - val_accuracy: 0.6329\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3857 - accuracy: 0.7122 - val_loss: 0.7384 - val_accuracy: 0.6299\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3857 - accuracy: 0.7107 - val_loss: 0.7219 - val_accuracy: 0.6299\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3858 - accuracy: 0.7108 - val_loss: 0.7353 - val_accuracy: 0.6269\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3848 - accuracy: 0.7123 - val_loss: 0.7478 - val_accuracy: 0.6284\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3846 - accuracy: 0.7110 - val_loss: 0.7319 - val_accuracy: 0.6329\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3854 - accuracy: 0.7119 - val_loss: 0.7203 - val_accuracy: 0.6201\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3854 - accuracy: 0.7121 - val_loss: 0.7366 - val_accuracy: 0.6336\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3844 - accuracy: 0.7121 - val_loss: 0.7398 - val_accuracy: 0.6284\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.3847 - accuracy: 0.7107 - val_loss: 0.7573 - val_accuracy: 0.6276\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3856 - accuracy: 0.7104 - val_loss: 0.7310 - val_accuracy: 0.6194\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3836 - accuracy: 0.7106 - val_loss: 0.7656 - val_accuracy: 0.6224\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3845 - accuracy: 0.7130 - val_loss: 0.7595 - val_accuracy: 0.6224\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3855 - accuracy: 0.7103 - val_loss: 0.7475 - val_accuracy: 0.6224\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3874 - accuracy: 0.7083 - val_loss: 0.7565 - val_accuracy: 0.6291\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3855 - accuracy: 0.7113 - val_loss: 0.7527 - val_accuracy: 0.6134\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3858 - accuracy: 0.7104 - val_loss: 0.7106 - val_accuracy: 0.6246\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3862 - accuracy: 0.7087 - val_loss: 0.7494 - val_accuracy: 0.6254\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3895 - accuracy: 0.7078 - val_loss: 0.7407 - val_accuracy: 0.6239\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3873 - accuracy: 0.7106 - val_loss: 0.7474 - val_accuracy: 0.6306\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.3880 - accuracy: 0.7078 - val_loss: 0.7096 - val_accuracy: 0.6239\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3884 - accuracy: 0.7099 - val_loss: 0.7157 - val_accuracy: 0.6261\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.3855 - accuracy: 0.7118 - val_loss: 0.7372 - val_accuracy: 0.6216\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3869 - accuracy: 0.7113 - val_loss: 0.7408 - val_accuracy: 0.6224\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 2s 88ms/step - loss: 0.3876 - accuracy: 0.7109 - val_loss: 0.7292 - val_accuracy: 0.6276\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3855 - accuracy: 0.7128 - val_loss: 0.7361 - val_accuracy: 0.6276\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3855 - accuracy: 0.7121 - val_loss: 0.7386 - val_accuracy: 0.6246\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3857 - accuracy: 0.7114 - val_loss: 0.7240 - val_accuracy: 0.6276\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3854 - accuracy: 0.7110 - val_loss: 0.7310 - val_accuracy: 0.6239\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3849 - accuracy: 0.7124 - val_loss: 0.7399 - val_accuracy: 0.6239\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3861 - accuracy: 0.7120 - val_loss: 0.7422 - val_accuracy: 0.6321\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3855 - accuracy: 0.7121 - val_loss: 0.7377 - val_accuracy: 0.6321\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3844 - accuracy: 0.7132 - val_loss: 0.7503 - val_accuracy: 0.6209\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3831 - accuracy: 0.7114 - val_loss: 0.7367 - val_accuracy: 0.6246\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3841 - accuracy: 0.7118 - val_loss: 0.7351 - val_accuracy: 0.6336\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3850 - accuracy: 0.7123 - val_loss: 0.7650 - val_accuracy: 0.6291\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3840 - accuracy: 0.7120 - val_loss: 0.7650 - val_accuracy: 0.6239\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3833 - accuracy: 0.7113 - val_loss: 0.7587 - val_accuracy: 0.6186\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3830 - accuracy: 0.7132 - val_loss: 0.7548 - val_accuracy: 0.6246\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3839 - accuracy: 0.7117 - val_loss: 0.7766 - val_accuracy: 0.6284\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3887 - accuracy: 0.7110 - val_loss: 0.7321 - val_accuracy: 0.6284\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3915 - accuracy: 0.7064 - val_loss: 0.7435 - val_accuracy: 0.6171\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3882 - accuracy: 0.7095 - val_loss: 0.7269 - val_accuracy: 0.6261\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3854 - accuracy: 0.7095 - val_loss: 0.7408 - val_accuracy: 0.6261\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3853 - accuracy: 0.7096 - val_loss: 0.7412 - val_accuracy: 0.6269\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3835 - accuracy: 0.7126 - val_loss: 0.7548 - val_accuracy: 0.6276\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3839 - accuracy: 0.7119 - val_loss: 0.7466 - val_accuracy: 0.6299\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3830 - accuracy: 0.7119 - val_loss: 0.7535 - val_accuracy: 0.6344\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3833 - accuracy: 0.7127 - val_loss: 0.7528 - val_accuracy: 0.6269\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3832 - accuracy: 0.7118 - val_loss: 0.7786 - val_accuracy: 0.6231\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3842 - accuracy: 0.7129 - val_loss: 0.7580 - val_accuracy: 0.6254\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3834 - accuracy: 0.7121 - val_loss: 0.7770 - val_accuracy: 0.6224\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3900 - accuracy: 0.7126 - val_loss: 0.7490 - val_accuracy: 0.6299\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4014 - accuracy: 0.7060 - val_loss: 0.6831 - val_accuracy: 0.6209\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3920 - accuracy: 0.7112 - val_loss: 0.7060 - val_accuracy: 0.6306\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3857 - accuracy: 0.7109 - val_loss: 0.7122 - val_accuracy: 0.6276\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3836 - accuracy: 0.7127 - val_loss: 0.7412 - val_accuracy: 0.6239\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3835 - accuracy: 0.7102 - val_loss: 0.7353 - val_accuracy: 0.6261\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3836 - accuracy: 0.7116 - val_loss: 0.7568 - val_accuracy: 0.6261\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3843 - accuracy: 0.7128 - val_loss: 0.7635 - val_accuracy: 0.6254\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3844 - accuracy: 0.7125 - val_loss: 0.7539 - val_accuracy: 0.6224\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3835 - accuracy: 0.7137 - val_loss: 0.7606 - val_accuracy: 0.6209\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3833 - accuracy: 0.7137 - val_loss: 0.7470 - val_accuracy: 0.6284\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3835 - accuracy: 0.7132 - val_loss: 0.7698 - val_accuracy: 0.6299\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3822 - accuracy: 0.7138 - val_loss: 0.7606 - val_accuracy: 0.6344\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3827 - accuracy: 0.7133 - val_loss: 0.7592 - val_accuracy: 0.6216\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3831 - accuracy: 0.7112 - val_loss: 0.7580 - val_accuracy: 0.6246\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3821 - accuracy: 0.7143 - val_loss: 0.7610 - val_accuracy: 0.6321\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3821 - accuracy: 0.7133 - val_loss: 0.7724 - val_accuracy: 0.6276\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3820 - accuracy: 0.7124 - val_loss: 0.7716 - val_accuracy: 0.6291\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3820 - accuracy: 0.7139 - val_loss: 0.7821 - val_accuracy: 0.6276\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3831 - accuracy: 0.7118 - val_loss: 0.7858 - val_accuracy: 0.6276\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3819 - accuracy: 0.7138 - val_loss: 0.7862 - val_accuracy: 0.6216\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3826 - accuracy: 0.7121 - val_loss: 0.7726 - val_accuracy: 0.6329\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3828 - accuracy: 0.7134 - val_loss: 0.7580 - val_accuracy: 0.6209\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3822 - accuracy: 0.7124 - val_loss: 0.7934 - val_accuracy: 0.6231\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 2s 89ms/step - loss: 0.3822 - accuracy: 0.7146 - val_loss: 0.7888 - val_accuracy: 0.6186\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3832 - accuracy: 0.7136 - val_loss: 0.7942 - val_accuracy: 0.6231\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3829 - accuracy: 0.7137 - val_loss: 0.7888 - val_accuracy: 0.6291\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.8423 - accuracy: 0.5958\n",
      "\n",
      "\n",
      "test_loss: 0.8422598242759705 test_accuracy: 0.5958083868026733\n"
     ]
    }
   ],
   "source": [
    "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
    "\n",
    "input1 = Input(shape=(16, ))\n",
    "input2 = Input(shape=(128,))\n",
    "embed1 = embeddingLayer(input1)\n",
    "embed2 = embeddingLayer(input2)\n",
    "lstm_1 = LSTM(100, return_sequences = False)(embed1)\n",
    "lstm_2 = LSTM(100, return_sequences = False)(embed2)\n",
    "\n",
    "dense1 = Dense(16)(lstm_1)\n",
    "dense2 = Dense(16)(lstm_2)\n",
    "concatenate = keras.layers.Concatenate()([dense1, dense2])\n",
    "output = Dense(3, activation = 'softmax')(concatenate)\n",
    "\n",
    "model3_2 = keras.models.Model(inputs=[input1, input2], outputs = output)\n",
    "model3_2.summary()\n",
    "\n",
    "model3_2.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "history_3_2 = model3_2.fit([x_train_aspect_pad_glove,\n",
    "                           x_train_review_pad_glove],\n",
    "                           y_train,\n",
    "                           epochs=200,\n",
    "                           batch_size=512,\n",
    "                           validation_data=([x_dev_aspect_pad_glove, x_dev_review_pad_glove], y_dev),\n",
    "                           verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "results_3_2 = model3_2.evaluate([x_test_aspect_pad_glove,\n",
    "                                 x_test_review_pad_glove], y_test)\n",
    "print('\\n')\n",
    "print('test_loss:', results_3_2[0], 'test_accuracy:', results_3_2[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_PPzd6R6gt8"
   },
   "source": [
    "#  Model 4: Another LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IC85cIumM9V"
   },
   "source": [
    "We can find that every aspect appears in the review sentence, which means we can extract the aspect information from the sentence. In most cases, the polarity of the aspect is determined by the content near it. Therefore, an LSTM can transfer the information of adjacent context to the aspect. We only need to extract the aspect vector to calculate its polarity, without analyzing the whole sentence.\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "(This is a bit like a simplified version of the \"unknown target\" sequence tagging models discussed in lectures - but we're only trying to tag the one part of the sequence that we know corresponds to the aspect mention.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1647002621454,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "pcxqucKN66sy",
    "outputId": "b3fbadaf-3d99-4623-c96d-c91b12ce84e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation error:\n",
      "['i love the food here, and although it is pricey, the entree comes with rice, naan, dal, and salad, which makes it worthwhile.', 'd al', 'neutral', '24', '28']\n",
      "\n",
      "\n",
      "\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['the scene there are two distinct personalities to the place: the loud, seemingly always-crowded bar with hanging paper decorations and dim lighting, and the two main dining areas, where the noise level and decor is notably more subdued.', 'noise level', 'negative', '190', '201']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def aspect_mask(reviews, aspects, dataset):\n",
    "    mask = []\n",
    "    for review,aspect,data in zip(reviews, aspects, dataset):\n",
    "        find_aspect = False\n",
    "        for j in range(5):\n",
    "            aspect_num = len(aspect)\n",
    "            aspect_str = \" \".join(aspect)\n",
    "            aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
    "            offset = 0\n",
    "            for i,r in enumerate(review):\n",
    "                if i + aspect_num <= len(review):\n",
    "                    r_context = \" \".join(review[i:i+aspect_num])\n",
    "                    if r_context == aspect_str and offset + aspect_len >  int(data[3]) and offset + aspect_len <    int(data[4]):\n",
    "                        find_aspect = True\n",
    "                        sentence_mask = [0] * len(review)\n",
    "                        sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
    "                        mask.append(sentence_mask)\n",
    "                        break\n",
    "                    else:\n",
    "                        offset += (len(r) + 1)\n",
    "            if find_aspect:\n",
    "                break\n",
    "\n",
    "        if not find_aspect:\n",
    "            for j in range(5):\n",
    "                aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
    "                offset = 0\n",
    "                for i,r in enumerate(review):\n",
    "                    if i + aspect_num <= len(review):\n",
    "                        r_context = \" \".join(review[i:i+aspect_num])\n",
    "                        if r_context.startswith(aspect_str) and offset + aspect_len >    int(data[3]) and offset + aspect_len <    int(data[4]):\n",
    "                            find_aspect = True\n",
    "                            sentence_mask = [0] * len(review)\n",
    "                            sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
    "                            mask.append(sentence_mask)\n",
    "                            break\n",
    "                        else:\n",
    "                            offset += (len(r) + 1)\n",
    "                if find_aspect:\n",
    "                    break\n",
    "\n",
    "        if not find_aspect:\n",
    "            for j in range(5):\n",
    "                aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
    "                offset = 0\n",
    "                for i,r in enumerate(review):\n",
    "                    if i + aspect_num <= len(review):\n",
    "                        r_context = \" \".join(review[i:i+aspect_num])\n",
    "                        if r_context.endswith(aspect_str) and offset + aspect_len >    int(data[3]) and offset + aspect_len <    int(data[4]):\n",
    "                            find_aspect = True\n",
    "                            sentence_mask = [0] * len(review)\n",
    "                            sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
    "                            mask.append(sentence_mask)\n",
    "                            break\n",
    "                        else:\n",
    "                            offset += (len(r) + 1)\n",
    "                if find_aspect:\n",
    "                    break\n",
    "\n",
    "        if not find_aspect:\n",
    "            print(\"annotation error:\")\n",
    "            print(data)\n",
    "            sentence_mask = [0] * len(review)\n",
    "            sentence_mask[16] = 1\n",
    "            mask.append(sentence_mask)\n",
    "\n",
    "        # if aspect_num > 1:\n",
    "        #     print(mask[-1])\n",
    "\n",
    "    return mask\n",
    "\n",
    "x_train_aspect_mask = aspect_mask(x_train_review, x_train_aspect, train)\n",
    "x_dev_aspect_mask = aspect_mask(x_dev_review, x_dev_aspect, val)\n",
    "x_test_aspect_mask = aspect_mask(x_test_review, x_test_aspect, test)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "assert len(x_train_aspect_mask) == len(train)\n",
    "assert len(x_test_aspect_mask) == len(x_test_aspect)\n",
    "\n",
    "print(train[0])\n",
    "print(x_train_aspect_mask[0])\n",
    "print(train[1])\n",
    "print(x_train_aspect_mask[1])\n",
    "print(train[2])\n",
    "print(x_train_aspect_mask[2])\n",
    "print(train[3])\n",
    "print(x_train_aspect_mask[3])\n",
    "print(train[10319])\n",
    "print(x_train_aspect_mask[10319])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1647002621454,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "PJK16El2yBQ0",
    "outputId": "01e224bb-84f9-4dc3-d26f-cf490b6347d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# although the name of function is review, it is not an error because of maxlen.\n",
    "\n",
    "x_train_aspect_mask_pad = review_pad_glove(x_train_aspect_mask)\n",
    "x_dev_aspect_mask_pad = review_pad_glove(x_dev_aspect_mask)\n",
    "x_test_aspect_mask_pad = review_pad_glove(x_test_aspect_mask)\n",
    "x_train_aspect_mask_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1647002621902,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "fn5N9P6y1ahO",
    "outputId": "12f3a572-410b-4594-d083-e3b46a8626da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " BiLSTM (Bidirectional)         (None, 128, 200)     320800      ['GloVe_Embeddings[6][0]']       \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 200)          0           ['BiLSTM[0][0]',                 \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 16)           3216        ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 3)            51          ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 120,324,367\n",
      "Trainable params: 324,067\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
    "\n",
    "input1 = Input(shape=(128,))\n",
    "embed = embeddingLayer(input1)\n",
    "bi_lstm = Bidirectional(LSTM(100, return_sequences=True),\n",
    "                        name = 'BiLSTM')(embed)\n",
    "input2 = Input(shape=(128,))\n",
    "dot = tf.keras.layers.Dot(axes=1)([bi_lstm, input2])\n",
    "dense2 = Dense(16)(dot)\n",
    "output = Dense(3, activation = 'softmax')(dense2)\n",
    "\n",
    "model_4 = keras.models.Model(inputs = [input1, input2], \n",
    "                            outputs = output)\n",
    "model_4.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1647002621903,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "f90SrYb4YHj3",
    "outputId": "cc178197-afd7-4a1b-fe85-1a4501b37338"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"392pt\" viewBox=\"0.00 0.00 786.00 470.00\" width=\"655pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 782,-466 782,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139822844727056 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139822844727056</title>\n",
       "<polygon fill=\"none\" points=\"50,-415.5 50,-461.5 380,-461.5 380,-415.5 50,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-446.3\">input_5</text>\n",
       "<polyline fill=\"none\" points=\"50,-438.5 130,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-423.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"130,-415.5 130,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"130,-438.5 188,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"188,-415.5 188,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236\" y=\"-434.8\">[(None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"284,-415.5 284,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332\" y=\"-434.8\">[(None, 128)]</text>\n",
       "</g>\n",
       "<!-- 139822879539472 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139822879539472</title>\n",
       "<polygon fill=\"none\" points=\"96,-332.5 96,-378.5 334,-378.5 334,-332.5 96,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-363.3\">GloVe_Embeddings</text>\n",
       "<polyline fill=\"none\" points=\"96,-355.5 230,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-340.3\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"230,-332.5 230,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"230,-355.5 288,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"288,-332.5 288,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"299.5\" y=\"-351.8\">?</text>\n",
       "<polyline fill=\"none\" points=\"311,-332.5 311,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-351.8\">?</text>\n",
       "</g>\n",
       "<!-- 139822844727056&#45;&gt;139822879539472 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139822844727056-&gt;139822879539472</title>\n",
       "<path d=\"M215,-415.3799C215,-407.1745 215,-397.7679 215,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"218.5001,-388.784 215,-378.784 211.5001,-388.784 218.5001,-388.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139827906737168 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139827906737168</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 430,-295.5 430,-249.5 0,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-280.3\">BiLSTM(lstm_4)</text>\n",
       "<polyline fill=\"none\" points=\"0,-272.5 138,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-257.3\">Bidirectional(LSTM)</text>\n",
       "<polyline fill=\"none\" points=\"138,-249.5 138,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"138,-272.5 196,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"196,-249.5 196,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-268.8\">(None, 128, 300)</text>\n",
       "<polyline fill=\"none\" points=\"313,-249.5 313,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-268.8\">(None, 128, 200)</text>\n",
       "</g>\n",
       "<!-- 139822879539472&#45;&gt;139827906737168 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139822879539472-&gt;139827906737168</title>\n",
       "<path d=\"M215,-332.3799C215,-324.1745 215,-314.7679 215,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"218.5001,-305.784 215,-295.784 211.5001,-305.784 218.5001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822859372944 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139822859372944</title>\n",
       "<polygon fill=\"none\" points=\"220.5,-166.5 220.5,-212.5 607.5,-212.5 607.5,-166.5 220.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-197.3\">dot</text>\n",
       "<polyline fill=\"none\" points=\"220.5,-189.5 258.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-174.3\">Dot</text>\n",
       "<polyline fill=\"none\" points=\"258.5,-166.5 258.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"258.5,-189.5 316.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"316.5,-166.5 316.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-185.8\">[(None, 128, 200), (None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"520.5,-166.5 520.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564\" y=\"-185.8\">(None, 200)</text>\n",
       "</g>\n",
       "<!-- 139827906737168&#45;&gt;139822859372944 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139827906737168-&gt;139822859372944</title>\n",
       "<path d=\"M270.1683,-249.4901C294.755,-239.2353 323.8812,-227.0872 349.4713,-216.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"350.8423,-219.6345 358.7244,-212.5547 348.1476,-213.1739 350.8423,-219.6345\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822844725648 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139822844725648</title>\n",
       "<polygon fill=\"none\" points=\"448,-249.5 448,-295.5 778,-295.5 778,-249.5 448,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-280.3\">input_6</text>\n",
       "<polyline fill=\"none\" points=\"448,-272.5 528,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-257.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"528,-249.5 528,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"528,-272.5 586,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"586,-249.5 586,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"634\" y=\"-268.8\">[(None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"682,-249.5 682,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-268.8\">[(None, 128)]</text>\n",
       "</g>\n",
       "<!-- 139822844725648&#45;&gt;139822859372944 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139822844725648-&gt;139822859372944</title>\n",
       "<path d=\"M557.8317,-249.4901C533.245,-239.2353 504.1188,-227.0872 478.5287,-216.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"479.8524,-213.1739 469.2756,-212.5547 477.1577,-219.6345 479.8524,-213.1739\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139822809872272 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139822809872272</title>\n",
       "<polygon fill=\"none\" points=\"266,-83.5 266,-129.5 562,-129.5 562,-83.5 266,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-114.3\">dense_12</text>\n",
       "<polyline fill=\"none\" points=\"266,-106.5 337,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-91.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"337,-83.5 337,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"337,-106.5 395,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"395,-83.5 395,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"438.5\" y=\"-102.8\">(None, 200)</text>\n",
       "<polyline fill=\"none\" points=\"482,-83.5 482,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"522\" y=\"-102.8\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 139822859372944&#45;&gt;139822809872272 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139822859372944-&gt;139822809872272</title>\n",
       "<path d=\"M414,-166.3799C414,-158.1745 414,-148.7679 414,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"417.5001,-139.784 414,-129.784 410.5001,-139.784 417.5001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139827903115344 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139827903115344</title>\n",
       "<polygon fill=\"none\" points=\"273.5,-.5 273.5,-46.5 554.5,-46.5 554.5,-.5 273.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-31.3\">dense_13</text>\n",
       "<polyline fill=\"none\" points=\"273.5,-23.5 344.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-8.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"344.5,-.5 344.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"344.5,-23.5 402.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"402.5,-.5 402.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442.5\" y=\"-19.8\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"482.5,-.5 482.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"518.5\" y=\"-19.8\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 139822809872272&#45;&gt;139827903115344 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>139822809872272-&gt;139827903115344</title>\n",
       "<path d=\"M414,-83.3799C414,-75.1745 414,-65.7679 414,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"417.5001,-56.784 414,-46.784 410.5001,-56.784 417.5001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import vis_utils\n",
    "SVG(vis_utils.model_to_dot(model_4, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60978,
     "status": "ok",
     "timestamp": 1647002682871,
     "user": {
      "displayName": "Jiahao Meng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiN8E1m4W7usRF7O7cdlqj0sKUK0l-ElyjNet6t-Q=s64",
      "userId": "16510454238886472053"
     },
     "user_tz": 0
    },
    "id": "opW6uOM81ahO",
    "outputId": "747dd41e-0db6-4ddb-e82f-41248c7c1d43",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "22/22 [==============================] - 8s 200ms/step - loss: 0.5817 - accuracy: 0.5263 - val_loss: 0.5183 - val_accuracy: 0.6494\n",
      "Epoch 2/16\n",
      "22/22 [==============================] - 3s 157ms/step - loss: 0.4933 - accuracy: 0.6560 - val_loss: 0.4732 - val_accuracy: 0.6809\n",
      "Epoch 3/16\n",
      "22/22 [==============================] - 3s 157ms/step - loss: 0.4447 - accuracy: 0.6930 - val_loss: 0.4459 - val_accuracy: 0.7102\n",
      "Epoch 4/16\n",
      "22/22 [==============================] - 3s 156ms/step - loss: 0.4122 - accuracy: 0.7230 - val_loss: 0.4283 - val_accuracy: 0.7155\n",
      "Epoch 5/16\n",
      "22/22 [==============================] - 3s 155ms/step - loss: 0.3813 - accuracy: 0.7465 - val_loss: 0.4274 - val_accuracy: 0.7282\n",
      "Epoch 6/16\n",
      "22/22 [==============================] - 3s 155ms/step - loss: 0.3564 - accuracy: 0.7685 - val_loss: 0.4184 - val_accuracy: 0.7350\n",
      "Epoch 7/16\n",
      "22/22 [==============================] - 3s 155ms/step - loss: 0.3303 - accuracy: 0.7879 - val_loss: 0.4158 - val_accuracy: 0.7312\n",
      "Epoch 8/16\n",
      "22/22 [==============================] - 3s 156ms/step - loss: 0.3037 - accuracy: 0.8085 - val_loss: 0.4259 - val_accuracy: 0.7297\n",
      "Epoch 9/16\n",
      "22/22 [==============================] - 3s 157ms/step - loss: 0.2821 - accuracy: 0.8256 - val_loss: 0.4397 - val_accuracy: 0.7417\n",
      "Epoch 10/16\n",
      "22/22 [==============================] - 3s 156ms/step - loss: 0.2567 - accuracy: 0.8444 - val_loss: 0.4499 - val_accuracy: 0.7290\n",
      "Epoch 11/16\n",
      "22/22 [==============================] - 3s 158ms/step - loss: 0.2406 - accuracy: 0.8569 - val_loss: 0.4505 - val_accuracy: 0.7387\n",
      "Epoch 12/16\n",
      "22/22 [==============================] - 3s 156ms/step - loss: 0.2115 - accuracy: 0.8788 - val_loss: 0.4854 - val_accuracy: 0.7297\n",
      "Epoch 13/16\n",
      "22/22 [==============================] - 3s 156ms/step - loss: 0.1916 - accuracy: 0.8923 - val_loss: 0.5134 - val_accuracy: 0.7260\n",
      "Epoch 14/16\n",
      "22/22 [==============================] - 3s 155ms/step - loss: 0.1696 - accuracy: 0.9094 - val_loss: 0.5787 - val_accuracy: 0.7125\n",
      "Epoch 15/16\n",
      "22/22 [==============================] - 3s 157ms/step - loss: 0.1530 - accuracy: 0.9216 - val_loss: 0.5889 - val_accuracy: 0.7230\n",
      "Epoch 16/16\n",
      "22/22 [==============================] - 3s 157ms/step - loss: 0.1255 - accuracy: 0.9406 - val_loss: 0.6081 - val_accuracy: 0.7237\n",
      "42/42 [==============================] - 1s 18ms/step - loss: 0.5812 - accuracy: 0.7260\n",
      "\n",
      "\n",
      "test_loss: 0.5812343955039978 test_accuracy: 0.726047933101654\n"
     ]
    }
   ],
   "source": [
    "model_4.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history_4 = model_4.fit([x_train_review_pad_glove, \n",
    "                         x_train_aspect_mask_pad],\n",
    "                       y_train,\n",
    "                       epochs=16,\n",
    "                       batch_size=512,\n",
    "                       validation_data=([x_dev_review_pad_glove, \n",
    "                                         x_dev_aspect_mask_pad], \n",
    "                                        y_dev),\n",
    "                       verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "results_4 = model_4.evaluate([x_test_review_pad_glove,\n",
    "                              x_test_aspect_mask_pad], y_test)\n",
    "print('\\n')\n",
    "print('test_loss:', results_4[0], 'test_accuracy:', results_4[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3CuAHDGQ3Mmp",
    "iafDTygK28fv",
    "7GdY2-64YG1B",
    "4RyeDPimMW7c",
    "vdZ4nl08vp9A",
    "ExgX8bxpVgps",
    "K0npTvFuVt5R"
   ],
   "machine_shape": "hm",
   "name": "lab4_Aspect-Based-Sentiment Analysis_without_answers.ipynb",
   "provenance": [
    {
     "file_id": "1EIAXsGRD8-EEk5abAYprUZAQjIVCx9Ia",
     "timestamp": 1644586931263
    },
    {
     "file_id": "1TmkpJzi0mA6QhKvfEjm70G2_shoi003g",
     "timestamp": 1644313818757
    },
    {
     "file_id": "1XylMCzA348Jss0muxzp7onr2Jn8E7_r6",
     "timestamp": 1613716607622
    }
   ]
  },
  "kernelspec": {
   "display_name": "Py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
